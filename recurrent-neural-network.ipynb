{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%writefile rotation.py\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nfrom trajectories import Trajectories\ntest_traject = 5\n\ntraject = Trajectories()\nXs, y, y_inicial = traject.generate_train_test_trajects(test_traject)\n\ndef rotate(tra, ang):\n    \n    theta = (ang/180) * np.pi\n    device = torch.device (\"cuda:0\" if torch.cuda.is_available () else \"cpu\")\n    \n    \n\n    rot_matrix = np.array([[np.cos(theta), -np.sin(theta)], \n                     [np.sin(theta),  np.cos(theta)]])\n    \n    r = torch.zeros_like(tra).cpu().detach().numpy()\n    print(r.ndim)\n    if r.ndim == 3:\n        for k in range(0, tra.size()[0]):\n            tra1 = tra.cpu().detach().numpy()\n            r[k,:,:] = tra1[k,:,:]@rot_matrix\n    else:\n            tra1 = tra.cpu().detach().numpy()\n            r[:,:] = tra1[:,:]@rot_matrix\n    return r\n\n# y2 = rotate(y, 90)\n# y_inicial2 = rotate(y_inicial, 180)\n\n# fig, [ax1, ax2] = plt.subplots(nrows=1, ncols=2)\n\n# ax1.plot(y[0,:,0].cpu().detach().numpy(), y[0,:,1].cpu().detach().numpy(), c = 'r', label = 'original', marker = '.')\n# plt.legend()\n\n# ax2.plot(y2[0,:,0], y2[0,:,1], c = 'b', label = 'rotated', marker = '.')\n# plt.legend()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile trajectories.py\nimport numpy as np\nimport gc\nimport torch\nimport torch.nn as nn\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\nmini_batch_size = 128\nN_mini_batch = 130\ntotal_traject = mini_batch_size * N_mini_batch\ntrain_traject = int(total_traject * 0.7)\ntest_traject = int(total_traject * 0.3)\nN_despl = 20\nN_coord = 2\nlimit_arena = 1\nlimit_arena2 = -1\ngrad_90 = np.pi\n\nclass Trajectories:\n\n    def generate_train_test_trajects(self, type_traject):\n        device = torch.device (\"cuda:0\" if torch.cuda.is_available () else \"cpu\")\n        print(f'Is cuda available in trajectories.py?: {torch.cuda.is_available ()}')\n        \n        Xs_t = torch.zeros([type_traject,N_despl,N_coord], device=device) \n        y_t = torch.zeros([type_traject,N_despl,N_coord], device=device)\n        y_inicial = torch.zeros([type_traject,N_coord], device=device)\n\n#         Xs_t = Xs_t.to (device)\n#         y_t = y_t.to (device)\n#         y_inicial = y_inicial.to (device)\n        \n        print(f'Device cuda for y_inicial in trajectories.py:{y_inicial.is_cuda}')\n        for k in range(type_traject):\n            d360 = np.pi * 2\n            ranX = np.zeros(N_despl)  \n            des = np.ones(N_despl)*0.05\n\n            for j in range(N_despl):\n                if j == 0:\n                    ranX[j] = float(np.random.uniform(0, d360, 1))\n                else:\n                    #ranX[j] = 0.95*ranX[j-1] + 0.05*(np.pi - float(np.random.uniform(0, d360, 1)))\n                    ranX[j] = ranX[j-1] + float(np.random.uniform(-d360*0.1, d360*0.1, 1))\n\n\n            cos = np.cos(ranX)\n\n            sin = np.sin(ranX)\n\n            ranY = []\n            for i in range(len(cos)):\n                ranY.append(des[i] / cos[i])\n\n            Xak = []\n\n\n            for i in range(len(ranX)):\n                list1 = []\n                list1.append(cos[i]*des[i])\n                list1.append(sin[i]*des[i])\n                Xak.append(list1)\n\n            ylistk = []\n            inicial = []\n\n            for i in range(len(ranX)):\n                coords = []\n                if i == 0:\n                    x_cond_in = float(np.random.uniform(limit_arena2+np.abs(Xak[i][0]), limit_arena-np.abs(Xak[i][0]), 1))\n                    y_cond_in = float(np.random.uniform(limit_arena2+np.abs(Xak[i][1]), limit_arena-np.abs(Xak[i][1]), 1))\n                    coords.append(Xak[i][0] + x_cond_in)\n                    coords.append(Xak[i][1] + y_cond_in)\n                    ylistk.append(coords)  # posiciones iniciales en x e y\n                    inicial.append(x_cond_in)\n                    inicial.append(y_cond_in)\n                else:\n                    x_coord = cos[i]*des[i] + ylistk[i - 1][0]\n                    y_coord = sin[i]*des[i] + ylistk[i - 1][1]\n                    if limit_arena2 < y_coord < limit_arena and limit_arena2 < x_coord < limit_arena:\n                        coords.append (x_coord)\n                        coords.append (y_coord)\n                        ylistk.append (coords)\n                    elif x_coord <= limit_arena2 or x_coord >= limit_arena:\n                        x_coord = ylistk[i - 1][0]\n                        y_coord = ylistk[i - 1][1]\n                        cos[i:] = np.cos (ranX[i:] + grad_90)\n                        sin[i:] = np.sin (ranX[i:] + grad_90)\n                        coords.append (x_coord)\n                        coords.append (y_coord)\n                        ylistk.append (coords)\n                    elif y_coord <= limit_arena2 or y_coord >= limit_arena:\n                        x_coord = ylistk[i - 1][0]\n                        y_coord = ylistk[i - 1][1]\n                        cos[i:] = np.cos (ranX[i:] + grad_90)\n                        sin[i:] = np.sin (ranX[i:] + grad_90)\n                        coords.append (x_coord)\n                        coords.append (y_coord)\n                        ylistk.append (coords)\n                    #     coords.append(x_coord)\n                    #     coords.append(y_coord)\n                    #     ylistk.append(coords)\n                    # elif x_coord <= limit_arena2:\n                    #     x_coord = limit_arena\n                    #     if y_coord < limit_arena2:\n                    #         y_coord = limit_arena\n                    #     if y_coord > limit_arena:\n                    #         y_coord = limit_arena2\n                    #     coords.append(x_coord)\n                    #     coords.append(y_coord)\n                    #     ylistk.append(coords)\n                    # elif x_coord >= limit_arena:\n                    #     x_coord = limit_arena2\n                    #     if y_coord < limit_arena2:\n                    #         y_coord = limit_arena\n                    #     if y_coord > limit_arena:\n                    #         y_coord = limit_arena2\n                    #     coords.append(x_coord)\n                    #     coords.append(y_coord)\n                    #     ylistk.append(coords)\n                    # elif y_coord <= limit_arena2:\n                    #     y_coord = limit_arena\n                    #     coords.append(x_coord)\n                    #     coords.append(y_coord)\n                    #     ylistk.append(coords)\n                    # elif y_coord >= limit_arena:\n                    #     y_coord = limit_arena2\n                    #     coords.append(x_coord)\n                    #     coords.append(y_coord)\n                    #     ylistk.append(coords)\n\n\n            Xsk = torch.tensor([Xak], device=device)\n            yk = torch.tensor([ylistk], device=device)\n            inicial = torch.tensor([inicial], device=device)\n\n            y_inicial[k,:] = inicial\n            Xs_t[k,:,:] = Xsk\n            y_t[k,:,:] = yk\n\n            \n            Xs = Xs_t\n            y = y_t      \n            \n\n        return Xs, y, y_inicial\nprint(\"trajectories\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile model.py\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\nN_despl = 2\nN_input_neurons = 4\nN_hidden_neurons = 100\nN_softmax_neurons = 200\n\nclass NeuralNetwork(nn.Module):\n            \n    def __init__(self):\n        super(NeuralNetwork, self).__init__()\n\n        self.linear = nn.Linear(N_input_neurons, N_hidden_neurons, bias=False) #probar otra inicializacion, xavier\n        nn.init.xavier_uniform_(self.linear.weight, gain=0.9)\n        #nn.init.zeros_(self.linear.weight)\n        self.Tanh = nn.ReLU() \n\n        self.M = torch.randn((N_despl, N_hidden_neurons))*0.01\n        self.M = torch.nn.Parameter(self.M)\n\n\n        self.linear2 = nn.Linear(N_hidden_neurons, N_hidden_neurons, bias=False)\n        nn.init.orthogonal_(self.linear2.weight, gain=0.9)\n        #nn.init.zeros_(self.linear2.weight)\n        self.Tanh2 = nn.ReLU()\n        \n        \n\n        self.linear4 = nn.Linear(N_hidden_neurons, N_softmax_neurons, bias=False)\n        nn.init.uniform_(self.linear4.weight, -0.1, 0.1)\n        #nn.init.zeros_(self.linear4.weight)\n\n        #self.dropout = nn.Dropout(p=0.70)\n        \n        self.linear5 = nn.Linear(N_softmax_neurons, N_hidden_neurons, bias=False)\n        nn.init.zeros_(self.linear5.weight)\n\n        self.softmax = nn.Softmax(dim=1)\n\n        self.linear3 = nn.Linear(N_softmax_neurons, N_despl, bias=False)\n        nn.init.uniform_(self.linear3.weight, -0.1, 0.1)\n        #nn.init.zeros_(self.linear3.weight) \n\n\n        \n    def forward(self, input, x0, xretro):\n         \n        xin = self.linear(input)\n        #noise =  torch.empty(x0.size(), device='cuda:0').normal_(mean=0,std=0.7)\n\n        u0 = self.Tanh(x0)\n        xrec = self.linear2(u0)       \n        dx = (-x0 + xin + xrec + xretro) / 10 \n        x1 = x0 + dx\n        \n        def add_noise(weights, noise):                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n            weights.add_(noise)\n        \n        #r = 0.4\n        #noise_linear2 = torch.randn(x1.size(), device='cuda:0')*r \n        \n        \n        #add_noise(x1, noise_linear2) #acotar para que el ruido complique \n         \n        #adam diferente const aprendizaje para cada parametro    \n        \n        x2 = self.Tanh(x1)\n        x2c = torch.clamp(x2, min=None, max=1)\n\n        #x2cd = self.dropout(x2c)\n        \n        x3 = self.linear4(x2c)\n        beta = 0.8\n        x4 = x3*beta\n        x5 = self.softmax(x4)\n        \n        xretro = self.linear5(x5)\n        \n        s = 0.02\n        noise_linear3 = torch.randn(x5.size(), device='cuda:0')*s\n        #add_noise(x5, noise_linear3)\n\n        x6 = x5 + noise_linear3\n        u1 = self.Tanh(x5)  \n        yd = self.linear3(u1)\n        \n\n        return yd, x1, x3, xretro\n    \n    #mas neuronas\n    #retroconexion de softmax a recurrente\n    #division por norma de vector en lugar de softmax\n    \n    #batch-normalization? +mu / sigma\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport numpy as np\ndevice = torch.device (\"cpu\")\npermutationX = torch.randperm(10)\nhalf = int(permutationX.detach().numpy().size/2)\npermutationX2 = permutationX[0:half]\npermutationX3 = permutationX[half:]\nprint(permutationX)\nprint(permutationX2)\nprint(permutationX3)\np = torch.cat((permutationX2, permutationX3))\n\nprint(p)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def distancia_modelo_real(tensor1, tensor2):\n    x_distance = np.mean (np.abs ((tensor1[:, 0] - tensor2[:, 0])) / (np.max (tensor1[:, 0]) - np.min (tensor1[:, 0])))\n    y_distance = np.mean (np.abs ((tensor1[:, 1] - tensor2[:, 1])) / (np.max (tensor1[:, 1]) - np.min (tensor1[:, 1])))\n    return x_distance, y_distance\nprint('done')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile test.py\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom model import NeuralNetwork\nfrom trajectories import Trajectories\nfrom rotation import rotate\n\nmini_batch_size = 128\nN_mini_batch = 130\ntest_traject = 256\nN_despl = 20\nN_coord = 2\nlimit_arena = 1\nlimit_arena2 = -1\ngrad_90 = np.pi\nN_despl = 20\nN_coord = 2\nN_hidden_neurons = 100\nN_softmax_neurons = 200\n\nbeta = 0.8\n\n\n\nclass Testing:\n\n    \n    def test_model(self):\n        \n        device = torch.device (\"cuda:0\" if torch.cuda.is_available () else \"cpu\")\n\n        my_network = NeuralNetwork()\n        my_network.to(device)\n        my_network.M.to (device)\n        \n        \n        mseloss = nn.MSELoss()\n        #optimizer = torch.optim.Adam(my_network.parameters(), lr = 0.01)\n        \n        traject = Trajectories()\n\n        torch.save(my_network,'/kaggle/working/MyNetworkKaggleTest7.tar')\n        \n        batch_y1 = torch.zeros(test_traject,N_despl,N_coord, device=device)\n        y2 = torch.zeros(test_traject,N_despl,N_coord, device=device)\n        \n        all_activation_test = torch.zeros(test_traject, N_despl, N_hidden_neurons, device=device)\n        all_activation_soft_test = torch.zeros(test_traject, N_despl, N_softmax_neurons, device=device)\n        \n        Xs, y, y_inicial = traject.generate_train_test_trajects(test_traject)\n        \n        \n        y_rot = rotate(y, 180)\n        y_inicial_rot = rotate(y_inicial, 180)\n        \n        y_rot1 = torch.from_numpy(y_rot)\n        y_rot1 = y_rot1.to(device)\n        y_inicial_rot1 = torch.from_numpy(y_inicial_rot)\n        y_inicial_rot1 = y_inicial_rot1.to(device)\n        \n        my_network.eval()\n        \n        with torch.inference_mode():\n            permutationX = torch.randperm(Xs.size()[0], device=device)\n            test_batch = 128\n            \n            for i in range(0,Xs.size()[0], test_batch): #iterate minibatch\n\n                activation = torch.zeros ((test_batch,N_despl, N_hidden_neurons), device=device)\n                activation_soft = torch.zeros ((test_batch,N_despl, N_softmax_neurons), device=device)\n\n\n                # remove current gradients for next iteration\n                #optimizer.zero_grad(set_to_none=True)\n        \n                firings = torch.zeros((test_batch, N_hidden_neurons), device=device)\n                #firings = firings.to(device)\n                firings_soft = torch.zeros((test_batch, N_softmax_neurons), device=device)\n                #firings_soft = firings_soft.to(device)\n\n                indicesX = permutationX[i:i + test_batch]\n                batch_x, batch_y = Xs[indicesX], y_rot1[indicesX]\n                batch_x = batch_x.to(device)\n                vect = y_inicial_rot1[indicesX]\n                vect = vect.cuda()\n                \n                x0 = vect@my_network.M\n                xretro = torch.zeros((test_batch, N_hidden_neurons), device=device)\n                \n                ytotal = torch.zeros(test_batch,N_despl,2, device=device)\n                for k in range(0, Xs.size()[1]):  # iterate time\n\n                    # input training example and return the prediction\n                    yhat, x0, x3, xretro = my_network.forward(batch_x[:,k,:], x0, xretro)\n                    ytotal[:,k,:] = yhat\n\n                    firing = torch.relu(x0)\n                    activation[:,k,:] = firing\n\n                    firing2 = x3\n                    firing2 *= beta\n                    firing_soft = torch.softmax(firing2, dim=1)\n                    activation_soft[:,k,:] = firing_soft\n\n                batch_y1[i:i + test_batch,:,:] = batch_y.detach()\n                y2[i:i + test_batch,:,:] = ytotal.detach()\n\n\n                all_activation_test[i:i + test_batch,:,:] = activation.detach()\n                all_activation_soft_test[i:i + test_batch,:,:] = activation_soft.detach()\n\n\n                # calculate MSE loss\n                # coef = 0.05\n                loss1 = mseloss (ytotal, batch_y)\n                # loss2 = firings_mean\n                # loss = loss1 * (1 - coef) + loss2 * coef\n                #\n                # # append to loss\n                # current_loss += loss\n        return loss1, all_activation_test, all_activation_soft_test, batch_y1, y2, my_network\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile train.py\nimport os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom model import NeuralNetwork\nfrom trajectories import Trajectories\nfrom rotation import rotate\n\nmini_batch_size = 128\nN_mini_batch = 130\ntotal_traject = mini_batch_size * N_mini_batch\ntrain_traject = int(total_traject * 0.7)\ntest_traject = int(total_traject * 0.3)\nN_despl = 20\nN_coord = 2\nN_input_neurons = 4\nN_hidden_neurons = 100\nN_softmax_neurons = 200\nlimit_arena = 1\nlimit_arena2 = -1\ngrad_90 = np.pi\n\nbeta = 0.8\n\n\ndef add_noise(weights, noise):                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n    weights.add_(noise)  \n\nclass Training:\n    \n    \n    \n    def train_model(self):\n        device = torch.device (\"cuda:0\" if torch.cuda.is_available () else \"cpu\")\n        print(f'Is cuda available in train.py?: {torch.cuda.is_available ()}')\n\n        my_network = NeuralNetwork()\n#         my_network.to(device)\n#         my_network.M.to (device)\n        \n        #my_network = torch.load('/kaggle/working/MyNetworkKaggle5.tar')\n        my_network.to(device)\n        my_network.M.to (device)\n        epochs = 1000\n        \n        \n        mseloss = nn.MSELoss()\n        \n        optimizer = torch.optim.Adam(my_network.parameters(), lr = 0.01)\n        \n        \n#         optimizer = torch.optim.Adam(\n#                         [\n#                             {\"params\": my_network.linear.parameters(), \"lr\": 1e-3},\n#                             {\"params\": my_network.linear3.parameters(), \"lr\": 1e-3},\n#                             {\"params\": my_network.linear4.parameters(), \"lr\": 1e-3},\n#                             {\"params\": my_network.linear2.parameters(), \"lr\": 1e-3},\n#                             {\"params\": my_network.linear5.parameters(), \"lr\": 1e-3},\n#                         ],\n#                         lr=1e-3,\n#                    )\n        \n        \n        all_losses = torch.zeros(epochs, device=device)\n        #all_losses.to(device)\n#         all_losses1 = torch.zeros(epochs)\n#         all_losses2 = torch.zeros(epochs)\n        traject = Trajectories()\n        Xs, y, y_inicial = traject.generate_train_test_trajects(train_traject)\n        \n        y_rot = rotate(y, 180)\n        y_inicial_rot = rotate(y_inicial, 180)\n        \n        y_rot1 = torch.from_numpy(y_rot)\n        y_rot1 = y_rot1.to(device)\n        y_inicial_rot1 = torch.from_numpy(y_inicial_rot)\n        y_inicial_rot1 = y_inicial_rot1.to(device)\n        \n        \n        \n        \n        \n        \n        \n        epoch = 0\n        \n    \n\n        plot_every = 100\n        N_hidden_neurons = 100\n        N_softmax_neurons = 200\n        N_input_neurons = 4\n        batch_y1 = torch.zeros(train_traject,N_despl,N_coord, device=device)\n        y2 = torch.zeros(train_traject,N_despl,N_coord, device=device)\n        \n        all_activation = torch.zeros(train_traject, N_despl, N_hidden_neurons, device=device)\n        all_activation_soft = torch.zeros(train_traject, N_despl, N_softmax_neurons, device=device)\n        \n    \n    \n#         my_network.load_state_dict(checkpoint['model_state_dict'])\n#         optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n#         epoch = checkpoint['epoch']\n#         loss = checkpoint['loss']\n#         my_network.state_dict\n\n        # peso = checkpoint['model_state_dict']['linear2.weight']\n        # ones_diag = torch.ones(N_hidden_neurons, N_hidden_neurons)\n        # ones_diag = torch.tril(ones_diag, k=75)\n        # ones_diag = torch.triu(ones_diag, k=25)\n        # peso_d = ones_diag * peso\n        # checkpoint['model_state_dict']['linear2.weight'] = peso_d\n        #50 diagonales\n\n        # ones_diag = torch.ones (N_hidden_neurons, N_hidden_neurons)\n        # ones_diag = torch.tril (ones_diag, diagonal=3)\n        # ones_diag = torch.triu (ones_diag, diagonal=-3)\n\n        ## disable cudnn fixed the mem leak problem\n        torch.backends.cudnn.enabled = False\n        prev_mem = 0\n        pid = os.getpid()\n        \n        context_signal = torch.tensor([[1, 0], [0, 1]], device=device)\n        \n        for ep in range(epoch, epoch+epochs):\n\n            current_loss = 0\n#             current_loss1 = 0\n#             current_loss2 = 0\n            permutationX = torch.randperm(Xs.size()[0], device=device)\n            #permutationX = permutationX.to(device)\n#             counter_batchs = 0\n\n            for i in range(0,Xs.size()[0], mini_batch_size): #iterate minibatch N_mini_batch times\n\n                activation = torch.zeros ((mini_batch_size,N_despl, N_hidden_neurons), device=device)\n                activation_soft = torch.zeros ((mini_batch_size,N_despl, N_softmax_neurons), device=device)\n\n\n                # remove current gradients for next iteration\n                optimizer.zero_grad(set_to_none=True)\n        \n                firings = torch.zeros((mini_batch_size, N_hidden_neurons), device=device)\n                #firings = firings.to(device)\n                firings_soft = torch.zeros((mini_batch_size, N_softmax_neurons), device=device)\n                #firings_soft = firings_soft.to(device)\n\n                indicesX = permutationX[i:i + mini_batch_size]\n                \n                half = int(indicesX.cpu().detach().numpy().size/2)\n                indicesX2 = indicesX[0:half]\n                indicesX3 = indicesX[half:]\n                \n                \n                batch_y2 = y[indicesX2]\n                batch_y3 = y_rot1[indicesX3]\n\n                batch_y = torch.cat((batch_y2, batch_y3), dim=0)\n                \n                batch_x2 = Xs[indicesX2]\n                batch_x3 = Xs[indicesX3]\n                context_signal2 = context_signal[0].repeat(batch_x2.size()[0], batch_x2.size()[1], 1)\n                context_signal3 = context_signal[1].repeat(batch_x3.size()[0], batch_x3.size()[1], 1)\n                \n#                 print(f'context_signal[0].size()={context_signal[0].size()}')\n#                 print(f'context_signal[0]={context_signal[0]}')\n#                 print(f'context_signal2.size()={context_signal2.size()}')\n#                 print(f'batch_x2.size()={batch_x2.size()}')\n                \n                batch_x2 = torch.cat((batch_x2, context_signal2), dim=2)\n                batch_x3 = torch.cat((batch_x3, context_signal3), dim=2)\n                \n                batch_x = torch.cat((batch_x2, batch_x3), dim=0)\n                \n#                 print(f'batch_x.size()={batch_x.size()}')\n#                 print(f'batch_x elemento={batch_x[0,:,:]}')\n                \n                vect2 = y_inicial[indicesX2]\n                vect3 = y_inicial_rot1[indicesX3]\n                \n                vect = torch.cat((vect2, vect3), dim=0)\n                \n                #input nuevo, one hot que marca cada contexto en nuevas neuronas de capa de entrada\n                \n                #probar cambiar los limites del bineado del histograma para evitar el exceso en ciertos angulos\n                \n                # 128,2   x    2,100\n                x0 = vect@my_network.M\n                xretro = torch.zeros((mini_batch_size, N_hidden_neurons), device=device)\n\n                ytotal = torch.zeros(mini_batch_size,N_despl,2, device=device)\n                #ytotal = ytotal.to(device)\n                for k in range(0, Xs.size()[1]):  # iterate time\n\n                    # input training example and return the prediction\n\n                    yhat, x0, x3, xretro = my_network.forward(batch_x[:,k,:], x0, xretro)\n                    ytotal[:,k,:] = yhat\n                    \n                    firing = torch.relu(x0)\n                    activation[:,k,:] = firing\n\n                    firing2 = x3\n                    firing2 *= beta\n                    firing_soft = torch.softmax(firing2, dim=1)\n                    activation_soft[:,k,:] = firing_soft\n                    \n                    #firings += firing\n#                 print(f'Device cuda for ytotal in train.py:{ytotal.is_cuda}')\n#                 print(f'Device cuda for batch_y in train.py:{batch_y.is_cuda}')\n                \n                batch_y1[i:i + mini_batch_size,:,:] = batch_y.detach()\n                y2[i:i + mini_batch_size,:,:] = ytotal.detach()\n            \n            \n                all_activation[i:i + mini_batch_size,:,:] = activation.detach()\n                all_activation_soft[i:i + mini_batch_size,:,:] = activation_soft.detach()\n\n                # calculate MSE loss\n\n                # coef = 0.0001\n                # firings_mean = torch.sum(firings) / (N_despl*N_hidden_neurons)\n                with torch.cuda.amp.autocast():\n                    loss1 = mseloss(ytotal, batch_y) # - var\n                    #loss2 = firings_mean\n                    loss = loss1 #* (1-coef) + loss2 * coef\n                \n                # backpropogate through the loss gradiants\n                loss.backward()\n\n                # update model weights   \n                optimizer.step()\n                \n                # with torch.no_grad():\n                #     peso = my_network.linear2.weight\n                #     peso_d = torch.nn.Parameter(ones_diag * peso)\n                #     my_network.linear2.weight = peso_d\n                # 50 diagonales\n                # np.triu\n\n\n                # append to loss   \n                current_loss += loss.cpu().detach().numpy()\n                #current_loss1 += loss1.detach ().numpy ()\n                #current_loss2 += loss2.detach().numpy()\n\n            all_losses[ep-epoch] = current_loss\n            #all_losses1.append(current_loss1 / N_mini_batch)\n            #all_losses2.append(current_loss2 / N_mini_batch)\n\n\n\n            # print progress\n            if ep != 0 and ep % plot_every == 0:\n                print(f'Epoch: {ep} completed')\n                print(current_loss)\n                # loss1 = [loss.detach().numpy() for loss in all_losses]\n                # plt.plot(loss1)\n                # plt.ylabel('Loss')\n                # plt.xlabel('Epoch')\n                # plt.show()\n                cur_mem = (int(open('/proc/%s/statm'%pid, 'r').read().split()[1])+0.0)/256\n                add_mem = cur_mem - prev_mem\n                prev_mem = cur_mem\n                print(f'added mem: {add_mem}')\n                #print(my_network.linear4.weight.cpu().detach().numpy())\n\n        torch.save(my_network,'/kaggle/working/MyNetworkKaggle2.tar')\n\n#         torch.save({\n#                     'epoch': epoch,\n#                     'model_state_dict': my_network.state_dict(),\n#                     'optimizer_state_dict': optimizer.state_dict(),\n#                     'loss': current_loss,\n#                     }, '/kaggle/working/MyNetworkKaggle2.tar')\n\n        return all_losses, all_activation, all_activation_soft, batch_y1, y2, my_network\nprint(\"train\")\n\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from test import Testing\nimport numpy as np\nN_despl = 20\nN_coord = 2\nN_hidden_neurons = 100\nN_softmax_neurons = 200\ntest_traject = 256\nlimit_arena = 1\nlimit_arena2 = -1\n\nt_traject = test_traject\n        \ntest = Testing()\nloss1, all_activation_test, all_activation_soft_test, batch_y1, y2, my_network  = test.test_model()\n\nlinear_size = t_traject * N_despl\n\ndata_x = batch_y1[:,:, 0].cpu().detach().numpy().T\ndata_y = batch_y1[:,:, 1].cpu().detach().numpy().T\n# data[20,224] or [32,20] tiempo,N_traject\n\n\ndata_x = np.reshape(data_x, -1, order='F')\ndata_y = np.reshape(data_y, -1, order='F')\n\nhist, *edges = np.histogram2d(data_x, data_y, bins=20, range=[[limit_arena2, limit_arena], [limit_arena2, limit_arena]])\ninx = np.digitize(data_x, edges[0], right=True)\niny = np.digitize(data_y, edges[1], right=True)\n\naxisbins = list(zip(inx, iny))\naxisbins = np.array(axisbins)\n\n\nact_array_test = np.array(all_activation_test.cpu().detach().numpy())\nact_array1 = np.reshape(act_array_test, [linear_size, N_hidden_neurons], order = 'C')\n\nfinal_array = np.zeros([20,20,N_hidden_neurons])\ncounter = np.zeros([20,20])\ncounter = np.expand_dims(counter, 2)\n\nfor n in range(linear_size):\n    xbin = axisbins[n,0]\n    ybin = axisbins[n,1]\n    final_array[xbin-1,ybin-1,:] += act_array1[n,:]\n    counter[xbin-1,ybin-1] += 1\n    \nprint(f'loss = {loss1}')\nprint('done')\n\nfinal_array3 = final_array / counter\n\nimport matplotlib.pyplot as plt\n\nN_hidden_neurons = 100\nplt.figure(figsize=(20, 17))\nfor n in range(N_hidden_neurons):\n    ax = plt.subplot(10, 10, n + 1)\n    ax.imshow(final_array3[:,:,n])\nprint('Hidden layer')\nplt.savefig('hidden_neurons.png')","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import matplotlib.pyplot as plt\n\n# N_hidden_neurons = 100\n# plt.figure(figsize=(20, 17))\n# for n in range(N_hidden_neurons):\n#     ax = plt.subplot(10, 10, n + 1)\n#     ax.imshow(final_array3[:,:,n], vmin=0, vmax=1)\nprint('Hidden layer')\nplt.savefig('hidden_neurons.png')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\npeso = my_network.linear3.weight\npeso1 = peso.cpu().detach().numpy()\nplt.plot(peso1.T[:,0], peso1.T[:,1], \".\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# peso = my_network.linear5.weight\n# peso1 = peso.cpu().detach().numpy()\n# plt.plot(peso1.T[:,0], peso1.T[:,1], \".\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# act_array_test_soft = np.array(all_activation_soft_test.cpu().detach().numpy())\n# act_array_soft1 = np.reshape(act_array_test_soft, [linear_size, N_softmax_neurons], order = 'C')\n\n# final_array_soft = np.zeros([20,20,N_softmax_neurons])\n# counter = np.zeros([20,20])\n# counter = np.expand_dims(counter, 2)\n\n# for n in range(linear_size):\n#     xbin = axisbins[n,0]\n#     ybin = axisbins[n,1]\n#     final_array_soft[xbin-1,ybin-1,:] += act_array_soft1[n,:]\n#     counter[xbin-1,ybin-1] += 1\n\n# final_array_soft3 = final_array_soft / counter   \n    \n# import matplotlib.pyplot as plt\n\n# N_softmax_neurons = 200\n# plt.figure(figsize=(20, 17))\n# for n in range(N_softmax_neurons):\n#     ax = plt.subplot(20, 10, n + 1)\n#     ax.imshow(final_array_soft3[:,:,n])\n# print('Softmax layer')\n# plt.savefig('softmax_neurons.png')\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import matplotlib.pyplot as plt\n# import torch\n# import numpy as np\n# r = 0.4\n# noise_linear2  = torch.empty(1000, device='cuda:0').normal_(mean=0,std=0.7)/10\n# plt.hist(noise_linear2.cpu().detach().numpy())\n# print(np.mean(noise_linear2.cpu().detach().numpy()))\n# print('var')\n# print(np.std(noise_linear2.cpu().detach().numpy()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport torch.nn as nn\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\nfrom train import Training\n\nmini_batch_size = 128 #intentar 32,64 \nN_mini_batch = 130\ntotal_traject = mini_batch_size * N_mini_batch\ntrain_traject = int(total_traject * 0.7)\ntest_traject = int(total_traject * 0.3)\nN_despl = 20\nN_coord = 2\nlimit_arena = 1\nlimit_arena2 = -1\ngrad_90 = np.pi\n\nimport time\nstart_time = time.time()\nt_traject = train_traject\n## disable cudnn fixed the mem leak problem\ntorch.backends.cudnn.enabled = False\n        \ntrain = Training()\nall_losses, all_activation, all_activation_soft, batch_y1, y2, my_network  = train.train_model()\n#all_losses1, all_activation_soft\n# fig, ax = plt.subplots (2, 2)\n# ax[0, 0].plot (all_losses[1:], c='b', label='total')\n# ax[0, 0].set_title ('total loss')\n# ax[0, 1].plot (all_losses1[1:], c='g', label='MSEloss')\n# ax[0, 1].set_title ('MSEloss')\n# ax[1, 0].plot (all_losses2[1:], c='r', label='firing')\n# ax[1, 0].set_title ('firing')\n# plt.show ()\n\ntorch.save(my_network,'/kaggle/working/MyNetworkKaggle5.tar')\n\nlinear_size = t_traject * N_despl\n\ndata_x = batch_y1[:,:, 0].cpu().detach().numpy().T\ndata_y = batch_y1[:,:, 1].cpu().detach().numpy().T\n# data[20,224] or [32,20] tiempo,N_traject\n\n\ndata_x = np.reshape(data_x, -1, order='F')\ndata_y = np.reshape(data_y, -1, order='F')\nprint('done')\ntimedelta = time.time() - start_time\n\nprint(f\"The program took {timedelta} seconds to run\")\t","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"linear_size = t_traject * N_despl\n\ndata_x = batch_y1[:,:, 0].cpu().detach().numpy().T\ndata_y = batch_y1[:,:, 1].cpu().detach().numpy().T\n# data[20,224] or [32,20] tiempo,N_traject\n\n\ndata_x = np.reshape(data_x, -1, order='F')\ndata_y = np.reshape(data_y, -1, order='F')\n\nfig, ax = plt.subplots(1,1)\nax.hist2d(data_x, data_y, bins=20, range=[[limit_arena2, limit_arena], [limit_arena2, limit_arena]])\nplt.title('Todas las trayectorias')\nplt.savefig('all_trajects.png')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hist, *edges = np.histogram2d(data_x, data_y, bins=20, range=[[limit_arena2, limit_arena], [limit_arena2, limit_arena]])\ninx = np.digitize(data_x, edges[0], right=True)\niny = np.digitize(data_y, edges[1], right=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"axisbins = list(zip(inx, iny))\naxisbins = np.array(axisbins)\n\nfig, ax = plt.subplots(1,1)\nax.plot(y2[-1,:,0].cpu().detach().numpy(), y2[-1,:,1].detach().cpu().numpy(), c = 'b', label = 'modelo', marker = '.')\nax.plot(batch_y1[-1,:,0].cpu().detach().numpy(), batch_y1[-1,:,1].cpu().detach().numpy(), c = 'r', label = 'real', marker = '.')\nplt.legend()\nplt.savefig('traject_example.png')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"last_loss = all_losses[-1]\nprint(last_loss)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"diferencia_array = np.zeros((2, y2[:,0,0].cpu().detach().numpy().size))\n\nfor i in range(y2[:,0,0].cpu().detach().numpy().size):\n    x_dis, y_dis = distancia_modelo_real(y2[i, :, :].cpu().detach().numpy(), batch_y1[i, :, :].cpu().detach().numpy())\n    diferencia_array[:,i] = [x_dis, y_dis]\n\nplt.hist(diferencia_array[0,:], bins=100, range=[0, 1])\nplt.gca().set(title='Histograma de distancias', ylabel='Frequencia')\nplt.savefig('distance.png')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" N_hidden_neurons = 100\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.hist(np.log10(act_array1.flatten() + 0.00000001), bins=100, range=[-5,5])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.max(act_array1.flatten())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"act_array = np.array(all_activation.cpu().detach().numpy())\nact_array1 = np.reshape(act_array, [linear_size, N_hidden_neurons], order = 'C')\n\nfinal_array = np.zeros([20,20,N_hidden_neurons])\ncounter = np.zeros([20,20])\ncounter = np.expand_dims(counter, 2)\n\nfor n in range(linear_size):\n    xbin = axisbins[n,0]\n    ybin = axisbins[n,1]\n    final_array[xbin-1,ybin-1,:] += act_array1[n,:]\n    counter[xbin-1,ybin-1] += 1\n\nplt.plot(all_losses[1:].cpu().detach().numpy(), c = 'b', label = 'total')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.savefig('loss1.png')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, [[ax1, ax2], [ax3, ax4]] = plt.subplots(nrows=2, ncols=2)\n\nax1.plot(y2[0,:,0].cpu().detach().numpy(), y2[0,:,1].detach().cpu().numpy(), c = 'b', label = 'modelo', marker = '.')\nax1.plot(batch_y1[0,:,0].cpu().detach().numpy(), batch_y1[0,:,1].cpu().detach().numpy(), c = 'r', label = 'real', marker = '.')\nplt.legend()\n\nax2.plot(y2[1,:,0].cpu().detach().numpy(), y2[1,:,1].detach().cpu().numpy(), c = 'b', label = 'modelo', marker = '.')\nax2.plot(batch_y1[1,:,0].cpu().detach().numpy(), batch_y1[1,:,1].cpu().detach().numpy(), c = 'r', label = 'real', marker = '.')\nplt.legend()\n\nax3.plot(y2[2,:,0].cpu().detach().numpy(), y2[2,:,1].detach().cpu().numpy(), c = 'b', label = 'modelo', marker = '.')\nax3.plot(batch_y1[2,:,0].cpu().detach().numpy(), batch_y1[2,:,1].cpu().detach().numpy(), c = 'r', label = 'real', marker = '.')\nplt.legend()\n\nax4.plot(y2[3,:,0].cpu().detach().numpy(), y2[3,:,1].detach().cpu().numpy(), c = 'b', label = 'modelo', marker = '.')\nax4.plot(batch_y1[3,:,0].cpu().detach().numpy(), batch_y1[3,:,1].cpu().detach().numpy(), c = 'r', label = 'real', marker = '.')\nplt.legend()\nplt.savefig('traject_examples.png')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_y1[3,:,0].cpu().detach().numpy(), batch_y1[3,:,1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(all_losses[125:].cpu().detach().numpy(), c = 'b', label = 'total')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.savefig('loss.png')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"N_softmax_neurons = 200\nact_array_soft = np.array(all_activation_soft.cpu().detach().numpy())\nact_array2 = np.reshape(act_array_soft, [linear_size, N_softmax_neurons], order = 'C')\n\nfinal_array_soft = np.zeros([20,20,N_softmax_neurons])\ncounter_soft = np.zeros([20,20])\ncounter_soft = np.expand_dims(counter_soft, 2)\n\nfor n in range(linear_size):\n    xbin = axisbins[n,0]\n    ybin = axisbins[n,1]\n    final_array_soft[xbin-1,ybin-1,:] += act_array2[n,:]\n    counter_soft[xbin-1,ybin-1] += 1\nprint('done')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_array5 = final_array / counter\nfinal_array6 = final_array5[:,:,2]\nplt.imshow(final_array6)\nplt.colorbar()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_array6","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.hist(final_array[:,:,2])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"N_hidden_neurons = 100\nplt.figure(figsize=(20, 17))\nfor n in range(N_hidden_neurons):\n    ax = plt.subplot(10, 10, n + 1)\n    ax.imshow(final_array[:,:,n])\nprint('Hidden layer')\nplt.savefig('hidden_neurons.png')\n\n#Var(Time, batch)   activaciones recurrente\n#Ruido\n\n#Mayor beta, ruido en la softmax también, =/= n° neuronas","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_arrayplot = final_array / counter\nplt.figure(figsize=(20, 17))\nfor n in range(N_hidden_neurons):\n    ax = plt.subplot(10, 10, n + 1)\n    ax.imshow(final_arrayplot[:,:,n], vmin=0, vmax=1)\nprint('Hidden layer')\nplt.savefig('hidden_neurons1.png')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.hist(np.log10(act_array1.flatten() + 0.00000001), bins=100, range=[-5,5])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.max(act_array1.flatten())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"N_softmax_neurons = 200\nfinal_array_soft = final_array_soft / counter_soft\nfinal_array2 = final_array_soft[:,:,0]\nplt.figure(figsize=(40, 35))\nfor n in range(N_softmax_neurons):\n    ax = plt.subplot(20, 10, n + 1)\n    ax.imshow(final_array_soft[:,:,n])\nprint('Softmax layer')\nplt.savefig('softmax_neurons.png')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sum(final_array_soft[0,0,:])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_activation_soft.sum(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(final_array_soft[:,:,0], vmin=0, vmax=1)\nplt.colorbar()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sum(final_array_soft[0,0,:])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sum(counter_soft[0,0,:])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_array_soft = np.zeros([20,20,N_softmax_neurons])\ncounter_soft = np.zeros([20,20])\ncounter_soft = np.expand_dims(counter_soft, 2)\n\nfor n in range(linear_size):\n    xbin = axisbins[n,0]\n    ybin = axisbins[n,1]\n    final_array_soft[xbin-1,ybin-1,:] += act_array2[n,:]\n    counter_soft[xbin-1,ybin-1] += 1\nprint('done')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_array_softmax = final_array_soft / counter_soft\nplt.figure(figsize=(40, 35))\nfor n in range(N_softmax_neurons):\n    ax = plt.subplot(20, 10, n + 1)\n    ax.imshow(final_array_softmax[:,:,n], vmin=0, vmax=0.5)\nprint('Softmax layer')\nplt.savefig('softmax_neurons1.png')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# f = final_array_soft.flatten()\n# plt.hist(np.log10(f), bins=30)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plt.figure()\n# final_array2 = np.log10(final_array_soft[:,:,0])\n# plt.imshow(final_array2)\n# plt.colorbar()\n\n# plt.figure()\n# final_array3 = np.log10(final_array_soft[:,:,1])\n# plt.imshow(final_array3)\n# plt.colorbar()\n# print(np.max(np.abs(final_array_soft[:,:,0]-final_array_soft[:,:,1])))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from test import Testing\n# import numpy as np\n# N_despl = 20\n# N_coord = 2\n# N_hidden_neurons = 100\n# N_softmax_neurons = 200\n# test_traject = 256\n# limit_arena = 1\n# limit_arena2 = -1\n\n# t_traject = test_traject\n        \n# test = Testing()\n# loss1, all_activation_test, all_activation_soft_test, batch_y1, y2, my_network  = test.test_model()\n\n# linear_size = t_traject * N_despl\n\n# data_x = batch_y1[:,:, 0].cpu().detach().numpy().T\n# data_y = batch_y1[:,:, 1].cpu().detach().numpy().T\n# # data[20,224] or [32,20] tiempo,N_traject\n\n\n# data_x = np.reshape(data_x, -1, order='F')\n# data_y = np.reshape(data_y, -1, order='F')\n\n# hist, *edges = np.histogram2d(data_x, data_y, bins=20, range=[[limit_arena2, limit_arena], [limit_arena2, limit_arena]])\n# inx = np.digitize(data_x, edges[0], right=True)\n# iny = np.digitize(data_y, edges[1], right=True)\n\n# axisbins = list(zip(inx, iny))\n# axisbins = np.array(axisbins)\n\n\n# act_array_test = np.array(all_activation_test.cpu().detach().numpy())\n# act_array1 = np.reshape(act_array_test, [linear_size, N_hidden_neurons], order = 'C')\n\n# final_array = np.zeros([20,20,N_hidden_neurons])\n# counter = np.zeros([20,20])\n# counter = np.expand_dims(counter, 2)\n\n# for n in range(linear_size):\n#     xbin = axisbins[n,0]\n#     ybin = axisbins[n,1]\n#     final_array[xbin-1,ybin-1,:] += act_array1[n,:]\n#     counter[xbin-1,ybin-1] += 1\n    \n# print(f'loss = {loss1}')\n# print('done')\n\n# final_array3 = final_array / counter\n\n# import matplotlib.pyplot as plt\n\n# N_hidden_neurons = 100\n# plt.figure(figsize=(20, 17))\n# for n in range(N_hidden_neurons):\n#     ax = plt.subplot(10, 10, n + 1)\n#     ax.imshow(final_array3[:,:,n])\n# print('Hidden layer')\n# plt.savefig('hidden_neurons_rot.png')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import matplotlib.pyplot as plt\n\n# N_hidden_neurons = 100\n# plt.figure(figsize=(20, 17))\n# for n in range(N_hidden_neurons):\n#     ax = plt.subplot(10, 10, n + 1)\n#     ax.imshow(final_array3[:,:,n], vmin=0, vmax=1)\n# print('Hidden layer')\n# plt.savefig('hidden_neurons_rot2.png')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# act_array_test_soft = np.array(all_activation_soft_test.cpu().detach().numpy())\n# act_array_soft1 = np.reshape(act_array_test_soft, [linear_size, N_softmax_neurons], order = 'C')\n\n# final_array_soft = np.zeros([20,20,N_softmax_neurons])\n# counter = np.zeros([20,20])\n# counter = np.expand_dims(counter, 2)\n\n# for n in range(linear_size):\n#     xbin = axisbins[n,0]\n#     ybin = axisbins[n,1]\n#     final_array_soft[xbin-1,ybin-1,:] += act_array_soft1[n,:]\n#     counter[xbin-1,ybin-1] += 1\n\n# final_array_soft3 = final_array_soft / counter   \n    \n# import matplotlib.pyplot as plt\n\n# N_softmax_neurons = 200\n# plt.figure(figsize=(20, 17))\n# for n in range(N_softmax_neurons):\n#     ax = plt.subplot(20, 10, n + 1)\n#     ax.imshow(final_array_soft3[:,:,n])\n# print('Softmax layer')\n# plt.savefig('softmax_neurons_rot.png')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# peso = my_network.linear3.weight\n# peso1 = peso.cpu().detach().numpy()\n# plt.plot(peso1.T[:,0], peso1.T[:,10], \".\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"peso = my_network.linear3.weight\npeso2 = peso.cpu().detach().numpy().flatten()\nplt.hist(peso2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"peso = my_network.linear5.weight\npeso1 = peso.cpu().detach().numpy()\nplt.plot(peso1.T[:,0], peso1.T[:,1], \".\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"peso = my_network.linear5.weight\npeso1 = peso.cpu().detach().numpy().flatten()\nplt.hist(peso1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"peso = my_network.linear4.weight\npeso1 = peso.cpu().detach().numpy()\nplt.plot(peso1.T[:,0], peso1.T[:,1], \"x\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.min(peso1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"peso = my_network.linear.weight\npeso1 = peso.cpu().detach().numpy()\nplt.plot(peso1.T[0,:], peso1.T[0,:], \"x\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plt.figure(figsize=(20, 17))\n# plt.imshow(all_activation_soft[0,:,:].cpu().detach().numpy())\n# plt.colorbar(shrink=0.25)\n\n# plt.figure(figsize=(20, 17))\n# plt.imshow(all_activation_soft[1,:,:].cpu().detach().numpy())\n# plt.colorbar(shrink=0.25)\n\n# plt.figure(figsize=(20, 17))\n# plt.imshow(all_activation_soft[2,:,:].cpu().detach().numpy())\n# plt.colorbar(shrink=0.25)\n\n# plt.figure(figsize=(20, 17))\n# plt.imshow(all_activation_soft[3,:,:].cpu().detach().numpy())\n# plt.colorbar(shrink=0.25)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plt.figure(figsize=(20, 17))\n# plt.imshow(all_activation_soft[0,:,:].cpu().detach().numpy())\n# plt.colorbar(shrink=0.25)\n\n# fig, ax = plt.subplots(1,1)\n# ax.plot(y2[0,:,0].cpu().detach().numpy(), y2[0,:,1].detach().cpu().numpy(), c = 'b', label = 'modelo', marker = '.')\n# ax.plot(batch_y1[0,:,0].cpu().detach().numpy(), batch_y1[0,:,1].cpu().detach().numpy(), c = 'r', label = 'real', marker = '.')\n# plt.legend()\n# plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plt.figure(figsize=(20, 17))\n# plt.imshow(np.log10(all_activation_soft[0,:,:].cpu().detach().numpy()))\n# plt.colorbar(shrink=0.25)\n\n# plt.figure(figsize=(20, 17))\n# plt.imshow(np.log10(all_activation_soft[1,:,:].cpu().detach().numpy()))\n# plt.colorbar(shrink=0.25)\n\n# plt.figure(figsize=(20, 17))\n# plt.imshow(np.log10(all_activation_soft[2,:,:].cpu().detach().numpy()))\n# plt.colorbar(shrink=0.25)\n\n# plt.figure(figsize=(20, 17))\n# plt.imshow(np.log10(all_activation_soft[3,:,:].cpu().detach().numpy()))\n# plt.colorbar(shrink=0.25)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# fig, [[ax1, ax2], [ax3, ax4]] = plt.subplots(nrows=2, ncols=2)\n\n# ax1.plot(y2[0,:,0].cpu().detach().numpy(), y2[0,:,1].detach().cpu().numpy(), c = 'b', label = 'modelo', marker = '.')\n# ax1.plot(batch_y1[0,:,0].cpu().detach().numpy(), batch_y1[0,:,1].cpu().detach().numpy(), c = 'r', label = 'real', marker = '.')\n# plt.legend()\n\n# ax2.plot(y2[1,:,0].cpu().detach().numpy(), y2[1,:,1].detach().cpu().numpy(), c = 'b', label = 'modelo', marker = '.')\n# ax2.plot(batch_y1[1,:,0].cpu().detach().numpy(), batch_y1[1,:,1].cpu().detach().numpy(), c = 'r', label = 'real', marker = '.')\n# plt.legend()\n\n# ax3.plot(y2[2,:,0].cpu().detach().numpy(), y2[2,:,1].detach().cpu().numpy(), c = 'b', label = 'modelo', marker = '.')\n# ax3.plot(batch_y1[2,:,0].cpu().detach().numpy(), batch_y1[2,:,1].cpu().detach().numpy(), c = 'r', label = 'real', marker = '.')\n# plt.legend()\n\n# ax4.plot(y2[3,:,0].cpu().detach().numpy(), y2[3,:,1].detach().cpu().numpy(), c = 'b', label = 'modelo', marker = '.')\n# ax4.plot(batch_y1[3,:,0].cpu().detach().numpy(), batch_y1[3,:,1].cpu().detach().numpy(), c = 'r', label = 'real', marker = '.')\n# plt.legend()\n# plt.savefig('traject_examples.png')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, [[ax1, ax2], [ax3, ax4]] = plt.subplots(nrows=2, ncols=2)\n\nax1.plot(y2[50,:,0].cpu().detach().numpy(), y2[50,:,1].detach().cpu().numpy(), c = 'b', label = 'modelo', marker = '.')\nax1.plot(batch_y1[50,:,0].cpu().detach().numpy(), batch_y1[50,:,1].cpu().detach().numpy(), c = 'r', label = 'real', marker = '.')\nplt.legend()\n\nax2.plot(y2[51,:,0].cpu().detach().numpy(), y2[51,:,1].detach().cpu().numpy(), c = 'b', label = 'modelo', marker = '.')\nax2.plot(batch_y1[51,:,0].cpu().detach().numpy(), batch_y1[51,:,1].cpu().detach().numpy(), c = 'r', label = 'real', marker = '.')\nplt.legend()\n\nax3.plot(y2[52,:,0].cpu().detach().numpy(), y2[52,:,1].detach().cpu().numpy(), c = 'b', label = 'modelo', marker = '.')\nax3.plot(batch_y1[52,:,0].cpu().detach().numpy(), batch_y1[52,:,1].cpu().detach().numpy(), c = 'r', label = 'real', marker = '.')\nplt.legend()\n\nax4.plot(y2[53,:,0].cpu().detach().numpy(), y2[53,:,1].detach().cpu().numpy(), c = 'b', label = 'modelo', marker = '.')\nax4.plot(batch_y1[53,:,0].cpu().detach().numpy(), batch_y1[53,:,1].cpu().detach().numpy(), c = 'r', label = 'real', marker = '.')\nplt.legend()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plt.figure(figsize=(20, 17))\n# plt.imshow(all_activation_soft[1,:,:].cpu().detach().numpy())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plt.figure(figsize=(20, 17))\n# plt.imshow(all_activation_soft[5,:,:].cpu().detach().numpy())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# all_activation_soft[5,:,69].cpu().detach().numpy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fig, ax = plt.subplots(1,1)\n# ax.plot(y2[5,:,0].cpu().detach().numpy(), y2[5,:,1].detach().cpu().numpy(), c = 'b', label = 'modelo', marker = '.')\n# ax.plot(batch_y1[5,:,0].cpu().detach().numpy(), batch_y1[5,:,1].cpu().detach().numpy(), c = 'r', label = 'real', marker = '.')\n# plt.legend()\n# plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plt.figure(figsize=(20, 17))\n# plt.imshow(all_activation_soft[10,:,:].cpu().detach().numpy())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plt.figure(figsize=(20, 17))\n# plt.imshow(all_activation_soft[0,:,:].cpu().detach().numpy())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plt.figure(figsize=(20, 17))\n# plt.imshow(all_activation_soft[15,:,:].cpu().detach().numpy())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_activation_soft","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(all_activation_soft[20,:,:].cpu().detach().numpy(), vmin=0, vmax=1)\nfig, ax = plt.subplots(1,1)\nax.plot(y2[20,:,0].cpu().detach().numpy(), y2[20,:,1].detach().cpu().numpy(), c = 'b', label = 'modelo', marker = '.')\nax.plot(batch_y1[20,:,0].cpu().detach().numpy(), batch_y1[20,:,1].cpu().detach().numpy(), c = 'r', label = 'real', marker = '.')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_activation_soft[20,:,[33,34]]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20, 17))\nplt.imshow(all_activation_soft[120,:,:].cpu().detach().numpy(), vmin=0, vmax=1)\n\nplt.figure(figsize=(20, 17))\nplt.imshow(all_activation_soft[121,:,:].cpu().detach().numpy(), vmin=0, vmax=1)\n\nplt.figure(figsize=(20, 17))\nplt.imshow(all_activation_soft[122,:,:].cpu().detach().numpy(), vmin=0, vmax=1)\n\nplt.figure(figsize=(20, 17))\nplt.imshow(all_activation_soft[123,:,:].cpu().detach().numpy(), vmin=0, vmax=1)\n\nplt.figure(figsize=(20, 17))\nplt.imshow(all_activation_soft[124,:,:].cpu().detach().numpy(), vmin=0, vmax=1)\n\nplt.figure(figsize=(20, 17))\nplt.imshow(all_activation_soft[125,:,:].cpu().detach().numpy(), vmin=0, vmax=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}