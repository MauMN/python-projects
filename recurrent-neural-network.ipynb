{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%writefile rotation.py\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nfrom trajectories import Trajectories\ntest_traject = 5\n\ntraject = Trajectories()\nXs, y, y_inicial = traject.generate_train_test_trajects(test_traject)\n\ndef rotate(tra, ang):\n    \n    theta = (ang/180) * np.pi\n    device = torch.device (\"cuda:0\" if torch.cuda.is_available () else \"cpu\")\n    \n    \n\n    rot_matrix = np.array([[np.cos(theta), -np.sin(theta)], \n                     [np.sin(theta),  np.cos(theta)]])\n    \n    r = torch.zeros_like(tra).cpu().detach().numpy()\n    print(r.ndim)\n    if r.ndim == 3:\n        for k in range(0, tra.size()[0]):\n            tra1 = tra.cpu().detach().numpy()\n            r[k,:,:] = tra1[k,:,:]@rot_matrix\n    else:\n            tra1 = tra.cpu().detach().numpy()\n            r[:,:] = tra1[:,:]@rot_matrix\n    return r\n\n# y2 = rotate(y, 90)\n# y_inicial2 = rotate(y_inicial, 180)\n\n# fig, [ax1, ax2] = plt.subplots(nrows=1, ncols=2)\n\n# ax1.plot(y[0,:,0].cpu().detach().numpy(), y[0,:,1].cpu().detach().numpy(), c = 'r', label = 'original', marker = '.')\n# plt.legend()\n\n# ax2.plot(y2[0,:,0], y2[0,:,1], c = 'b', label = 'rotated', marker = '.')\n# plt.legend()","metadata":{"execution":{"iopub.status.busy":"2023-09-11T18:04:53.929463Z","iopub.execute_input":"2023-09-11T18:04:53.929923Z","iopub.status.idle":"2023-09-11T18:04:53.962872Z","shell.execute_reply.started":"2023-09-11T18:04:53.929841Z","shell.execute_reply":"2023-09-11T18:04:53.961751Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Writing rotation.py\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile trajectories.py\nimport numpy as np\nimport gc\nimport torch\nimport torch.nn as nn\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\nmini_batch_size = 128\nN_mini_batch = 130\ntotal_traject = mini_batch_size * N_mini_batch\ntrain_traject = int(total_traject * 0.7)\ntest_traject = int(total_traject * 0.3)\nN_despl = 20\nN_coord = 2\nlimit_arena = 1\nlimit_arena2 = -1\ngrad_90 = np.pi\n\nclass Trajectories:\n\n    def generate_train_test_trajects(self, type_traject):\n        device = torch.device (\"cuda:0\" if torch.cuda.is_available () else \"cpu\")\n        print(f'Is cuda available in trajectories.py?: {torch.cuda.is_available ()}')\n        \n        Xs_t = torch.zeros([type_traject,N_despl,N_coord], device=device) \n        y_t = torch.zeros([type_traject,N_despl,N_coord], device=device)\n        y_inicial = torch.zeros([type_traject,N_coord], device=device)\n\n#         Xs_t = Xs_t.to (device)\n#         y_t = y_t.to (device)\n#         y_inicial = y_inicial.to (device)\n        \n        print(f'Device cuda for y_inicial in trajectories.py:{y_inicial.is_cuda}')\n        for k in range(type_traject):\n            d360 = np.pi * 2\n            ranX = np.zeros(N_despl)  \n            des = np.ones(N_despl)*0.05\n\n            for j in range(N_despl):\n                if j == 0:\n                    ranX[j] = float(np.random.uniform(0, d360, 1))\n                else:\n                    #ranX[j] = 0.95*ranX[j-1] + 0.05*(np.pi - float(np.random.uniform(0, d360, 1)))\n                    ranX[j] = ranX[j-1] + float(np.random.uniform(-d360*0.1, d360*0.1, 1))\n\n\n            cos = np.cos(ranX)\n\n            sin = np.sin(ranX)\n\n            ranY = []\n            for i in range(len(cos)):\n                ranY.append(des[i] / cos[i])\n\n            Xak = []\n\n\n            for i in range(len(ranX)):\n                list1 = []\n                list1.append(cos[i]*des[i])\n                list1.append(sin[i]*des[i])\n                Xak.append(list1)\n\n            ylistk = []\n            inicial = []\n\n            for i in range(len(ranX)):\n                coords = []\n                if i == 0:\n                    x_cond_in = float(np.random.uniform(limit_arena2+np.abs(Xak[i][0]), limit_arena-np.abs(Xak[i][0]), 1))\n                    y_cond_in = float(np.random.uniform(limit_arena2+np.abs(Xak[i][1]), limit_arena-np.abs(Xak[i][1]), 1))\n                    coords.append(Xak[i][0] + x_cond_in)\n                    coords.append(Xak[i][1] + y_cond_in)\n                    ylistk.append(coords)  # posiciones iniciales en x e y\n                    inicial.append(x_cond_in)\n                    inicial.append(y_cond_in)\n                else:\n                    x_coord = cos[i]*des[i] + ylistk[i - 1][0]\n                    y_coord = sin[i]*des[i] + ylistk[i - 1][1]\n                    if limit_arena2 < y_coord < limit_arena and limit_arena2 < x_coord < limit_arena:\n                        coords.append (x_coord)\n                        coords.append (y_coord)\n                        ylistk.append (coords)\n                    elif x_coord <= limit_arena2 or x_coord >= limit_arena:\n                        x_coord = ylistk[i - 1][0]\n                        y_coord = ylistk[i - 1][1]\n                        cos[i:] = np.cos (ranX[i:] + grad_90)\n                        sin[i:] = np.sin (ranX[i:] + grad_90)\n                        coords.append (x_coord)\n                        coords.append (y_coord)\n                        ylistk.append (coords)\n                    elif y_coord <= limit_arena2 or y_coord >= limit_arena:\n                        x_coord = ylistk[i - 1][0]\n                        y_coord = ylistk[i - 1][1]\n                        cos[i:] = np.cos (ranX[i:] + grad_90)\n                        sin[i:] = np.sin (ranX[i:] + grad_90)\n                        coords.append (x_coord)\n                        coords.append (y_coord)\n                        ylistk.append (coords)\n                    #     coords.append(x_coord)\n                    #     coords.append(y_coord)\n                    #     ylistk.append(coords)\n                    # elif x_coord <= limit_arena2:\n                    #     x_coord = limit_arena\n                    #     if y_coord < limit_arena2:\n                    #         y_coord = limit_arena\n                    #     if y_coord > limit_arena:\n                    #         y_coord = limit_arena2\n                    #     coords.append(x_coord)\n                    #     coords.append(y_coord)\n                    #     ylistk.append(coords)\n                    # elif x_coord >= limit_arena:\n                    #     x_coord = limit_arena2\n                    #     if y_coord < limit_arena2:\n                    #         y_coord = limit_arena\n                    #     if y_coord > limit_arena:\n                    #         y_coord = limit_arena2\n                    #     coords.append(x_coord)\n                    #     coords.append(y_coord)\n                    #     ylistk.append(coords)\n                    # elif y_coord <= limit_arena2:\n                    #     y_coord = limit_arena\n                    #     coords.append(x_coord)\n                    #     coords.append(y_coord)\n                    #     ylistk.append(coords)\n                    # elif y_coord >= limit_arena:\n                    #     y_coord = limit_arena2\n                    #     coords.append(x_coord)\n                    #     coords.append(y_coord)\n                    #     ylistk.append(coords)\n\n\n            Xsk = torch.tensor([Xak], device=device)\n            yk = torch.tensor([ylistk], device=device)\n            inicial = torch.tensor([inicial], device=device)\n\n            y_inicial[k,:] = inicial\n            Xs_t[k,:,:] = Xsk\n            y_t[k,:,:] = yk\n\n            \n            Xs = Xs_t\n            y = y_t      \n            \n\n        return Xs, y, y_inicial\nprint(\"trajectories\")","metadata":{"execution":{"iopub.status.busy":"2023-09-11T18:04:53.964643Z","iopub.execute_input":"2023-09-11T18:04:53.965571Z","iopub.status.idle":"2023-09-11T18:04:53.975001Z","shell.execute_reply.started":"2023-09-11T18:04:53.965532Z","shell.execute_reply":"2023-09-11T18:04:53.973874Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Writing trajectories.py\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile model.py\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\nN_despl = 2\nN_input_neurons = 4\nN_hidden_neurons = 100\nN_softmax_neurons = 200\n\nclass NeuralNetwork(nn.Module):\n            \n    def __init__(self):\n        super(NeuralNetwork, self).__init__()\n\n        self.linear = nn.Linear(N_input_neurons, N_hidden_neurons, bias=False) #probar otra inicializacion, xavier\n        nn.init.xavier_uniform_(self.linear.weight, gain=0.9)\n        #nn.init.zeros_(self.linear.weight)\n        self.Tanh = nn.ReLU() \n\n        self.M = torch.randn((N_despl, N_hidden_neurons))*0.01\n        self.M = torch.nn.Parameter(self.M)\n\n\n        self.linear2 = nn.Linear(N_hidden_neurons, N_hidden_neurons, bias=False)\n        nn.init.orthogonal_(self.linear2.weight, gain=0.9)\n        #nn.init.zeros_(self.linear2.weight)\n        self.Tanh2 = nn.ReLU()\n        \n        \n\n        self.linear4 = nn.Linear(N_hidden_neurons, N_softmax_neurons, bias=False)\n        nn.init.uniform_(self.linear4.weight, -0.1, 0.1)\n        #nn.init.zeros_(self.linear4.weight)\n\n        #self.dropout = nn.Dropout(p=0.70)\n        \n        self.linear5 = nn.Linear(N_softmax_neurons, N_hidden_neurons, bias=False)\n        nn.init.zeros_(self.linear5.weight)\n\n        self.softmax = nn.Softmax(dim=1)\n\n        self.linear3 = nn.Linear(N_softmax_neurons, N_despl, bias=False)\n        nn.init.uniform_(self.linear3.weight, -0.1, 0.1)\n        #nn.init.zeros_(self.linear3.weight) \n\n\n        \n    def forward(self, input, x0, xretro):\n         \n        xin = self.linear(input)\n        #noise =  torch.empty(x0.size(), device='cuda:0').normal_(mean=0,std=0.7)\n\n        u0 = self.Tanh(x0)\n        xrec = self.linear2(u0)       \n        dx = (-x0 + xin + xrec + xretro) / 10 \n        x1 = x0 + dx\n        \n        def add_noise(weights, noise):                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n            weights.add_(noise)\n        \n        #r = 0.4\n        #noise_linear2 = torch.randn(x1.size(), device='cuda:0')*r \n        \n        \n        #add_noise(x1, noise_linear2) #acotar para que el ruido complique \n         \n        #adam diferente const aprendizaje para cada parametro    \n        \n        x2 = self.Tanh(x1)\n        x2c = torch.clamp(x2, min=None, max=1)\n\n        #x2cd = self.dropout(x2c)\n        \n        x3 = self.linear4(x2c)\n        beta = 0.8\n        x4 = x3*beta\n        x5 = self.softmax(x4)\n        \n        xretro = self.linear5(x5)\n        \n        s = 0.02\n        noise_linear3 = torch.randn(x5.size(), device='cuda:0')*s\n        #add_noise(x5, noise_linear3)\n\n        x6 = x5 + noise_linear3\n        u1 = self.Tanh(x5)  \n        yd = self.linear3(u1)\n        \n\n        return yd, x1, x3, xretro\n    \n    #mas neuronas\n    #retroconexion de softmax a recurrente\n    #division por norma de vector en lugar de softmax\n    \n    #batch-normalization? +mu / sigma\n","metadata":{"execution":{"iopub.status.busy":"2023-09-11T18:04:54.028596Z","iopub.execute_input":"2023-09-11T18:04:54.029540Z","iopub.status.idle":"2023-09-11T18:04:54.037911Z","shell.execute_reply.started":"2023-09-11T18:04:54.029485Z","shell.execute_reply":"2023-09-11T18:04:54.036502Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Writing model.py\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport numpy as np\ndevice = torch.device (\"cpu\")\npermutationX = torch.randperm(10)\nhalf = int(permutationX.detach().numpy().size/2)\npermutationX2 = permutationX[0:half]\npermutationX3 = permutationX[half:]\nprint(permutationX)\nprint(permutationX2)\nprint(permutationX3)\np = torch.cat((permutationX2, permutationX3))\n\nprint(p)","metadata":{"execution":{"iopub.status.busy":"2023-09-11T18:04:54.040193Z","iopub.execute_input":"2023-09-11T18:04:54.040936Z","iopub.status.idle":"2023-09-11T18:04:55.611034Z","shell.execute_reply.started":"2023-09-11T18:04:54.040895Z","shell.execute_reply":"2023-09-11T18:04:55.610075Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"tensor([3, 6, 1, 7, 8, 4, 5, 0, 2, 9])\ntensor([3, 6, 1, 7, 8])\ntensor([4, 5, 0, 2, 9])\ntensor([3, 6, 1, 7, 8, 4, 5, 0, 2, 9])\n","output_type":"stream"}]},{"cell_type":"code","source":"def distancia_modelo_real(tensor1, tensor2):\n    x_distance = np.mean (np.abs ((tensor1[:, 0] - tensor2[:, 0])) / (np.max (tensor1[:, 0]) - np.min (tensor1[:, 0])))\n    y_distance = np.mean (np.abs ((tensor1[:, 1] - tensor2[:, 1])) / (np.max (tensor1[:, 1]) - np.min (tensor1[:, 1])))\n    return x_distance, y_distance\nprint('done')","metadata":{"execution":{"iopub.status.busy":"2023-09-11T18:04:55.612727Z","iopub.execute_input":"2023-09-11T18:04:55.613157Z","iopub.status.idle":"2023-09-11T18:04:55.619631Z","shell.execute_reply.started":"2023-09-11T18:04:55.613131Z","shell.execute_reply":"2023-09-11T18:04:55.618959Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"done\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile test.py\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom model import NeuralNetwork\nfrom trajectories import Trajectories\nfrom rotation import rotate\n\nmini_batch_size = 128\nN_mini_batch = 130\ntest_traject = 256\nN_despl = 20\nN_coord = 2\nlimit_arena = 1\nlimit_arena2 = -1\ngrad_90 = np.pi\nN_despl = 20\nN_coord = 2\nN_hidden_neurons = 100\nN_softmax_neurons = 200\n\nbeta = 0.8\n\n\n\nclass Testing:\n\n    \n    def test_model(self):\n        \n        device = torch.device (\"cuda:0\" if torch.cuda.is_available () else \"cpu\")\n\n        my_network = NeuralNetwork()\n        my_network.to(device)\n        my_network.M.to (device)\n        \n        \n        mseloss = nn.MSELoss()\n        #optimizer = torch.optim.Adam(my_network.parameters(), lr = 0.01)\n        \n        traject = Trajectories()\n\n        torch.save(my_network,'/kaggle/working/MyNetworkKaggleTest7.tar')\n        \n        batch_y1 = torch.zeros(test_traject,N_despl,N_coord, device=device)\n        y2 = torch.zeros(test_traject,N_despl,N_coord, device=device)\n        \n        all_activation_test = torch.zeros(test_traject, N_despl, N_hidden_neurons, device=device)\n        all_activation_soft_test = torch.zeros(test_traject, N_despl, N_softmax_neurons, device=device)\n        \n        Xs, y, y_inicial = traject.generate_train_test_trajects(test_traject)\n        \n        \n        y_rot = rotate(y, 250)\n        y_inicial_rot = rotate(y_inicial, 250)\n        \n        y_rot1 = torch.from_numpy(y_rot)\n        y_rot1 = y_rot1.to(device)\n        y_inicial_rot1 = torch.from_numpy(y_inicial_rot)\n        y_inicial_rot1 = y_inicial_rot1.to(device)\n        \n        context_signal = torch.tensor([[1, 0], [0, 1]], device=device)\n        \n        my_network.eval()\n        \n        with torch.inference_mode():\n            permutationX = torch.randperm(Xs.size()[0], device=device)\n            test_batch = 128\n            \n            for i in range(0,Xs.size()[0], test_batch): #iterate minibatch\n\n                activation = torch.zeros ((test_batch,N_despl, N_hidden_neurons), device=device)\n                activation_soft = torch.zeros ((test_batch,N_despl, N_softmax_neurons), device=device)\n\n\n                # remove current gradients for next iteration\n                #optimizer.zero_grad(set_to_none=True)\n        \n                firings = torch.zeros((test_batch, N_hidden_neurons), device=device)\n                #firings = firings.to(device)\n                firings_soft = torch.zeros((test_batch, N_softmax_neurons), device=device)\n                #firings_soft = firings_soft.to(device)\n\n                indicesX = permutationX[i:i + test_batch]\n                half = int(indicesX.cpu().detach().numpy().size/2)\n                indicesX2 = indicesX[0:half]\n                indicesX3 = indicesX[half:]\n                \n                \n                batch_y2 = y[indicesX2]\n                batch_y3 = y_rot1[indicesX3]\n\n                batch_y = torch.cat((batch_y2, batch_y3), dim=0)\n                \n                batch_x2 = Xs[indicesX2]\n                batch_x3 = Xs[indicesX3]\n                context_signal2 = context_signal[0].repeat(batch_x2.size()[0], batch_x2.size()[1], 1)\n                context_signal3 = context_signal[1].repeat(batch_x3.size()[0], batch_x3.size()[1], 1)\n                \n#                 print(f'context_signal[0].size()={context_signal[0].size()}')\n#                 print(f'context_signal[0]={context_signal[0]}')\n#                 print(f'context_signal2.size()={context_signal2.size()}')\n#                 print(f'batch_x2.size()={batch_x2.size()}')\n                \n                batch_x2 = torch.cat((batch_x2, context_signal2), dim=2)\n                batch_x3 = torch.cat((batch_x3, context_signal3), dim=2)\n                \n                batch_x = torch.cat((batch_x2, batch_x3), dim=0)\n                \n#                 print(f'batch_x.size()={batch_x.size()}')\n#                 print(f'batch_x elemento={batch_x[0,:,:]}')\n                \n                vect2 = y_inicial[indicesX2]\n                vect3 = y_inicial_rot1[indicesX3]\n                \n                vect = torch.cat((vect2, vect3), dim=0)\n                \n                x0 = vect@my_network.M\n                xretro = torch.zeros((test_batch, N_hidden_neurons), device=device)\n                \n                ytotal = torch.zeros(test_batch,N_despl,2, device=device)\n                for k in range(0, Xs.size()[1]):  # iterate time\n\n                    # input training example and return the prediction\n                    yhat, x0, x3, xretro = my_network.forward(batch_x[:,k,:], x0, xretro)\n                    ytotal[:,k,:] = yhat\n\n                    firing = torch.relu(x0)\n                    activation[:,k,:] = firing\n\n                    firing2 = x3\n                    firing2 *= beta\n                    firing_soft = torch.softmax(firing2, dim=1)\n                    activation_soft[:,k,:] = firing_soft\n\n                batch_y1[i:i + test_batch,:,:] = batch_y.detach()\n                y2[i:i + test_batch,:,:] = ytotal.detach()\n\n\n                all_activation_test[i:i + test_batch,:,:] = activation.detach()\n                all_activation_soft_test[i:i + test_batch,:,:] = activation_soft.detach()\n\n\n                # calculate MSE loss\n                # coef = 0.05\n                loss1 = mseloss (ytotal, batch_y)\n                # loss2 = firings_mean\n                # loss = loss1 * (1 - coef) + loss2 * coef\n                #\n                # # append to loss\n                # current_loss += loss\n        return loss1, all_activation_test, all_activation_soft_test, batch_y1, y2, my_network\n\n","metadata":{"execution":{"iopub.status.busy":"2023-09-11T18:04:55.620823Z","iopub.execute_input":"2023-09-11T18:04:55.621391Z","iopub.status.idle":"2023-09-11T18:04:55.641514Z","shell.execute_reply.started":"2023-09-11T18:04:55.621365Z","shell.execute_reply":"2023-09-11T18:04:55.640624Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Writing test.py\n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile train.py\nimport os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom model import NeuralNetwork\nfrom trajectories import Trajectories\nfrom rotation import rotate\n\nmini_batch_size = 128\nhalf_minibatch = mini_batch_size/2\nN_mini_batch = 130\ntotal_traject = mini_batch_size * N_mini_batch\ntrain_traject = int(total_traject * 0.7)\ntrain_traject2 = train_traject/2\ntest_traject = int(total_traject * 0.3)\nN_despl = 20\nN_coord = 2\nN_input_neurons = 4\nN_hidden_neurons = 100\nN_softmax_neurons = 200\nlimit_arena = 1\nlimit_arena2 = -1\ngrad_90 = np.pi\n\nbeta = 0.8\n\n\ndef add_noise(weights, noise):                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n    weights.add_(noise)  \n\nclass Training:\n    \n    \n    \n    def train_model(self):\n        device = torch.device (\"cuda:0\" if torch.cuda.is_available () else \"cpu\")\n        print(f'Is cuda available in train.py?: {torch.cuda.is_available ()}')\n\n        my_network = NeuralNetwork()\n#         my_network.to(device)\n#         my_network.M.to (device)\n        \n        #my_network = torch.load('/kaggle/working/MyNetworkKaggle5.tar')\n        my_network.to(device)\n        my_network.M.to (device)\n        epochs = 500\n        \n        \n        mseloss = nn.MSELoss()\n        \n        optimizer = torch.optim.Adam(my_network.parameters(), lr = 0.01)\n        \n        \n#         optimizer = torch.optim.Adam(\n#                         [\n#                             {\"params\": my_network.linear.parameters(), \"lr\": 1e-3},\n#                             {\"params\": my_network.linear3.parameters(), \"lr\": 1e-3},\n#                             {\"params\": my_network.linear4.parameters(), \"lr\": 1e-3},\n#                             {\"params\": my_network.linear2.parameters(), \"lr\": 1e-3},\n#                             {\"params\": my_network.linear5.parameters(), \"lr\": 1e-3},\n#                         ],\n#                         lr=1e-3,\n#                    )\n        \n        \n        all_losses = torch.zeros(epochs, device=device)\n        #all_losses.to(device)\n#         all_losses1 = torch.zeros(epochs)\n#         all_losses2 = torch.zeros(epochs)\n        traject = Trajectories()\n        Xs, y, y_inicial = traject.generate_train_test_trajects(train_traject)\n        \n        y_rot = rotate(y, 250)\n        y_inicial_rot = rotate(y_inicial, 250)\n        \n        y_rot1 = torch.from_numpy(y_rot)\n        y_rot1 = y_rot1.to(device)\n        y_inicial_rot1 = torch.from_numpy(y_inicial_rot)\n        y_inicial_rot1 = y_inicial_rot1.to(device)\n        \n        \n        \n        \n        \n        \n        \n        epoch = 0\n        \n    \n\n        plot_every = 100\n        N_hidden_neurons = 100\n        N_softmax_neurons = 200\n        N_input_neurons = 4\n        batch_y1 = torch.zeros(train_traject,N_despl,N_coord, device=device)\n        y2 = torch.zeros(train_traject,N_despl,N_coord, device=device)\n        \n        all_activation1 = torch.zeros(train_traject2, N_despl, N_hidden_neurons, device=device)\n        all_activation2 = torch.zeros(train_traject2, N_despl, N_hidden_neurons, device=device)\n        all_activation_soft1 = torch.zeros(train_traject2, N_despl, N_softmax_neurons, device=device)\n        all_activation_soft2 = torch.zeros(train_traject2, N_despl, N_softmax_neurons, device=device)\n        \n    \n    \n#         my_network.load_state_dict(checkpoint['model_state_dict'])\n#         optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n#         epoch = checkpoint['epoch']\n#         loss = checkpoint['loss']\n#         my_network.state_dict\n\n        # peso = checkpoint['model_state_dict']['linear2.weight']\n        # ones_diag = torch.ones(N_hidden_neurons, N_hidden_neurons)\n        # ones_diag = torch.tril(ones_diag, k=75)\n        # ones_diag = torch.triu(ones_diag, k=25)\n        # peso_d = ones_diag * peso\n        # checkpoint['model_state_dict']['linear2.weight'] = peso_d\n        #50 diagonales\n\n        # ones_diag = torch.ones (N_hidden_neurons, N_hidden_neurons)\n        # ones_diag = torch.tril (ones_diag, diagonal=3)\n        # ones_diag = torch.triu (ones_diag, diagonal=-3)\n\n        ## disable cudnn fixed the mem leak problem\n        torch.backends.cudnn.enabled = False\n        prev_mem = 0\n        pid = os.getpid()\n        \n        context_signal = torch.tensor([[1, 0], [0, 1]], device=device)\n        \n        for ep in range(epoch, epoch+epochs):\n\n            current_loss = 0\n#             current_loss1 = 0\n#             current_loss2 = 0\n            permutationX = torch.randperm(Xs.size()[0], device=device)\n            #permutationX = permutationX.to(device)\n#             counter_batchs = 0\n\n            for i in range(0,Xs.size()[0], mini_batch_size): #iterate minibatch N_mini_batch times\n\n                activation1 = torch.zeros ((half_minibatch,N_despl, N_hidden_neurons), device=device)\n                activation2 = torch.zeros ((half_minibatch,N_despl, N_hidden_neurons), device=device)\n                activation_soft1 = torch.zeros ((half_minibatch,N_despl, N_softmax_neurons), device=device)\n                activation_soft2 = torch.zeros ((half_minibatch,N_despl, N_softmax_neurons), device=device)\n\n\n                # remove current gradients for next iteration\n                optimizer.zero_grad(set_to_none=True)\n        \n                firings = torch.zeros((mini_batch_size, N_hidden_neurons), device=device)\n                #firings = firings.to(device)\n                firings_soft = torch.zeros((mini_batch_size, N_softmax_neurons), device=device)\n                #firings_soft = firings_soft.to(device)\n\n                indicesX = permutationX[i:i + mini_batch_size]\n                \n                half = int(indicesX.cpu().detach().numpy().size/2)\n                indicesX2 = indicesX[0:half]\n                indicesX3 = indicesX[half:]\n                \n                \n                batch_y2 = y[indicesX2]\n                batch_y3 = y_rot1[indicesX3]\n\n                batch_y = torch.cat((batch_y2, batch_y3), dim=0)\n                \n                batch_x2 = Xs[indicesX2]\n                batch_x3 = Xs[indicesX3]\n                context_signal2 = context_signal[0].repeat(batch_x2.size()[0], batch_x2.size()[1], 1)\n                context_signal3 = context_signal[1].repeat(batch_x3.size()[0], batch_x3.size()[1], 1)\n                \n#                 print(f'context_signal[0].size()={context_signal[0].size()}')\n#                 print(f'context_signal[0]={context_signal[0]}')\n#                 print(f'context_signal2.size()={context_signal2.size()}')\n#                 print(f'batch_x2.size()={batch_x2.size()}')\n                \n                batch_x2 = torch.cat((batch_x2, context_signal2), dim=2)\n                batch_x3 = torch.cat((batch_x3, context_signal3), dim=2)\n                \n                batch_x = torch.cat((batch_x2, batch_x3), dim=0)\n                \n#                 print(f'batch_x.size()={batch_x.size()}')\n#                 print(f'batch_x elemento={batch_x[0,:,:]}')\n                \n                vect2 = y_inicial[indicesX2]\n                vect3 = y_inicial_rot1[indicesX3]\n                \n                vect = torch.cat((vect2, vect3), dim=0)\n                \n                #input nuevo, one hot que marca cada contexto en nuevas neuronas de capa de entrada\n                \n                #probar cambiar los limites del bineado del histograma para evitar el exceso en ciertos angulos\n                \n                # 128,2   x    2,100\n                x0 = vect@my_network.M\n                xretro = torch.zeros((mini_batch_size, N_hidden_neurons), device=device)\n\n                ytotal = torch.zeros(mini_batch_size,N_despl,2, device=device)\n                #ytotal = ytotal.to(device)\n                for k in range(0, Xs.size()[1]):  # iterate time\n\n                    # input training example and return the prediction\n\n                    yhat, x0, x3, xretro = my_network.forward(batch_x[:,k,:], x0, xretro)\n                    ytotal[:,k,:] = yhat\n                    \n                    firing = torch.relu(x0)\n                    \n                    activation1[:,k,:] = firing[0:half_minibatch,:]\n                    activation2[:,k,:] = firing[half_minibatch:,:]\n\n                    firing2 = x3\n                    firing2 *= beta\n                    firing_soft = torch.softmax(firing2, dim=1)\n                    \n                    activation_soft1[:,k,:] = firing_soft[0:half_minibatch]\n                    activation_soft2[:,k,:] = firing_soft[half_minibatch:]\n                    \n                    #firings += firing\n#                 print(f'Device cuda for ytotal in train.py:{ytotal.is_cuda}')\n#                 print(f'Device cuda for batch_y in train.py:{batch_y.is_cuda}')\n                \n                batch_y1[i:i + mini_batch_size,:,:] = batch_y.detach()\n                y2[i:i + mini_batch_size,:,:] = ytotal.detach()\n            \n            \n                all_activation1[i:i + half_minibach,:,:] = activation1.detach()\n                all_activation2[i + half_minibach:,:,:] = activation2.detach()\n                all_activation_soft1[i:i + half_minibach,:,:] = activation_soft1.detach()\n                all_activation_soft2[i + half_minibach:,:,:] = activation_soft2.detach()\n\n                # calculate MSE loss\n\n                # coef = 0.0001\n                # firings_mean = torch.sum(firings) / (N_despl*N_hidden_neurons)\n                with torch.cuda.amp.autocast():\n                    loss1 = mseloss(ytotal, batch_y) # - var\n                    #loss2 = firings_mean\n                    loss = loss1 #* (1-coef) + loss2 * coef\n                \n                # backpropogate through the loss gradiants\n                loss.backward()\n\n                # update model weights   \n                optimizer.step()\n                \n                # with torch.no_grad():\n                #     peso = my_network.linear2.weight\n                #     peso_d = torch.nn.Parameter(ones_diag * peso)\n                #     my_network.linear2.weight = peso_d\n                # 50 diagonales\n                # np.triu\n\n\n                # append to loss   \n                current_loss += loss.cpu().detach().numpy()\n                #current_loss1 += loss1.detach ().numpy ()\n                #current_loss2 += loss2.detach().numpy()\n\n            all_losses[ep-epoch] = current_loss\n            #all_losses1.append(current_loss1 / N_mini_batch)\n            #all_losses2.append(current_loss2 / N_mini_batch)\n\n\n\n            # print progress\n            if ep != 0 and ep % plot_every == 0:\n                print(f'Epoch: {ep} completed')\n                print(current_loss)\n                # loss1 = [loss.detach().numpy() for loss in all_losses]\n                # plt.plot(loss1)\n                # plt.ylabel('Loss')\n                # plt.xlabel('Epoch')\n                # plt.show()\n                cur_mem = (int(open('/proc/%s/statm'%pid, 'r').read().split()[1])+0.0)/256\n                add_mem = cur_mem - prev_mem\n                prev_mem = cur_mem\n                print(f'added mem: {add_mem}')\n                #print(my_network.linear4.weight.cpu().detach().numpy())\n\n        torch.save(my_network,'/kaggle/working/MyNetworkKaggle2.tar')\n\n#         torch.save({\n#                     'epoch': epoch,\n#                     'model_state_dict': my_network.state_dict(),\n#                     'optimizer_state_dict': optimizer.state_dict(),\n#                     'loss': current_loss,\n#                     }, '/kaggle/working/MyNetworkKaggle2.tar')\n\n        return all_losses, all_activation1, all_activation2, all_activation_soft1, all_activation_soft2, batch_y1, y2, my_network\nprint(\"train\")\n\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2023-09-11T18:04:55.643806Z","iopub.execute_input":"2023-09-11T18:04:55.644077Z","iopub.status.idle":"2023-09-11T18:04:55.662571Z","shell.execute_reply.started":"2023-09-11T18:04:55.644052Z","shell.execute_reply":"2023-09-11T18:04:55.661592Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Writing train.py\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from test import Testing\nimport numpy as np\nN_despl = 20\nN_coord = 2\nN_hidden_neurons = 100\nN_softmax_neurons = 200\ntest_traject = 256\nlimit_arena = 1\nlimit_arena2 = -1\n\nt_traject = test_traject\n        \ntest = Testing()\nloss1, all_activation_test, all_activation_soft_test, batch_y1, y2, my_network  = test.test_model()\n\nlinear_size = t_traject * N_despl\n\ndata_x = batch_y1[:,:, 0].cpu().detach().numpy().T\ndata_y = batch_y1[:,:, 1].cpu().detach().numpy().T\n# data[20,224] or [32,20] tiempo,N_traject\n\n\ndata_x = np.reshape(data_x, -1, order='F')\ndata_y = np.reshape(data_y, -1, order='F')\n\nhist, *edges = np.histogram2d(data_x, data_y, bins=20, range=[[-np.max(data_x), np.max(data_x)], [-np.max(data_y), np.max(data_y)]])\ninx = np.digitize(data_x, edges[0], right=True)\niny = np.digitize(data_y, edges[1], right=True)\n\naxisbins = list(zip(inx, iny))\naxisbins = np.array(axisbins)\n\n\nact_array_test = np.array(all_activation_test.cpu().detach().numpy())\nact_array1 = np.reshape(act_array_test, [linear_size, N_hidden_neurons], order = 'C')\n\nfinal_array = np.zeros([20,20,N_hidden_neurons])\ncounter = np.zeros([20,20])\ncounter = np.expand_dims(counter, 2)\n\nfor n in range(linear_size):\n    xbin = axisbins[n,0]\n    ybin = axisbins[n,1]\n    final_array[xbin-1,ybin-1,:] += act_array1[n,:]\n    counter[xbin-1,ybin-1] += 1\n    \nprint(f'loss = {loss1}')\nprint('done')\n\nfinal_array3 = final_array / counter\n\nimport matplotlib.pyplot as plt\n\nN_hidden_neurons = 100\nplt.figure(figsize=(20, 17))\nfor n in range(N_hidden_neurons):\n    ax = plt.subplot(10, 10, n + 1)\n    ax.imshow(final_array3[:,:,n])\nprint('Hidden layer')\nplt.savefig('hidden_neurons.png')","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-09-11T18:04:55.663655Z","iopub.execute_input":"2023-09-11T18:04:55.663945Z","iopub.status.idle":"2023-09-11T18:04:56.156217Z","shell.execute_reply.started":"2023-09-11T18:04:55.663921Z","shell.execute_reply":"2023-09-11T18:04:56.152425Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"trajectories\nIs cuda available in trajectories.py?: False\nDevice cuda for y_inicial in trajectories.py:False\nIs cuda available in trajectories.py?: False\nDevice cuda for y_inicial in trajectories.py:False\n3\n2\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_27/973994590.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTesting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mloss1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_activation_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_activation_soft_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmy_network\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mlinear_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt_traject\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mN_despl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/kaggle/working/test.py\u001b[0m in \u001b[0;36mtest_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m                     \u001b[0;31m# input training example and return the prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m                     \u001b[0myhat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxretro\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_network\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxretro\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m                     \u001b[0mytotal\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myhat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/kaggle/working/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, x0, xretro)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.02\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mnoise_linear3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cuda:0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0;31m#add_noise(x5, noise_linear3)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    208\u001b[0m                 \"multiprocessing, you must use the 'spawn' start method\")\n\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_cuda_getDeviceCount'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Torch not compiled with CUDA enabled\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_cudart\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m             raise AssertionError(\n","\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"],"ename":"AssertionError","evalue":"Torch not compiled with CUDA enabled","output_type":"error"}]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nN_hidden_neurons = 100\nplt.figure(figsize=(20, 17))\nfor n in range(N_hidden_neurons):\n    ax = plt.subplot(10, 10, n + 1)\n    ax.imshow(final_array3[:,:,n], vmin=0, vmax=1)\nprint('Hidden layer')\nplt.savefig('hidden_neurons.png')","metadata":{"execution":{"iopub.status.busy":"2023-09-11T18:04:56.157318Z","iopub.status.idle":"2023-09-11T18:04:56.157874Z","shell.execute_reply.started":"2023-09-11T18:04:56.157581Z","shell.execute_reply":"2023-09-11T18:04:56.157606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\npeso = my_network.linear3.weight\npeso1 = peso.cpu().detach().numpy()\nplt.plot(peso1.T[:,0], peso1.T[:,1], \".\")","metadata":{"execution":{"iopub.status.busy":"2023-09-11T18:04:56.159516Z","iopub.status.idle":"2023-09-11T18:04:56.159906Z","shell.execute_reply.started":"2023-09-11T18:04:56.159735Z","shell.execute_reply":"2023-09-11T18:04:56.159755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# peso = my_network.linear5.weight\n# peso1 = peso.cpu().detach().numpy()\n# plt.plot(peso1.T[:,0], peso1.T[:,1], \".\")","metadata":{"execution":{"iopub.status.busy":"2023-09-11T18:04:56.161503Z","iopub.status.idle":"2023-09-11T18:04:56.162020Z","shell.execute_reply.started":"2023-09-11T18:04:56.161770Z","shell.execute_reply":"2023-09-11T18:04:56.161800Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"act_array_test_soft = np.array(all_activation_soft_test.cpu().detach().numpy())\nact_array_soft1 = np.reshape(act_array_test_soft, [linear_size, N_softmax_neurons], order = 'C')\n\nfinal_array_soft = np.zeros([20,20,N_softmax_neurons])\ncounter = np.zeros([20,20])\ncounter = np.expand_dims(counter, 2)\n\nfor n in range(linear_size):\n    xbin = axisbins[n,0]\n    ybin = axisbins[n,1]\n    final_array_soft[xbin-1,ybin-1,:] += act_array_soft1[n,:]\n    counter[xbin-1,ybin-1] += 1\n\nfinal_array_soft3 = final_array_soft / counter   \n    \nimport matplotlib.pyplot as plt\n\nN_softmax_neurons = 200\nplt.figure(figsize=(20, 17))\nfor n in range(N_softmax_neurons):\n    ax = plt.subplot(20, 10, n + 1)\n    ax.imshow(final_array_soft3[:,:,n])\nprint('Softmax layer')\nplt.savefig('softmax_neurons.png')\n\n","metadata":{"execution":{"iopub.status.busy":"2023-09-11T18:04:56.163080Z","iopub.status.idle":"2023-09-11T18:04:56.163524Z","shell.execute_reply.started":"2023-09-11T18:04:56.163292Z","shell.execute_reply":"2023-09-11T18:04:56.163314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import matplotlib.pyplot as plt\n# import torch\n# import numpy as np\n# r = 0.4\n# noise_linear2  = torch.empty(1000, device='cuda:0').normal_(mean=0,std=0.7)/10\n# plt.hist(noise_linear2.cpu().detach().numpy())\n# print(np.mean(noise_linear2.cpu().detach().numpy()))\n# print('var')\n# print(np.std(noise_linear2.cpu().detach().numpy()))","metadata":{"execution":{"iopub.status.busy":"2023-09-11T18:04:56.165118Z","iopub.status.idle":"2023-09-11T18:04:56.165569Z","shell.execute_reply.started":"2023-09-11T18:04:56.165345Z","shell.execute_reply":"2023-09-11T18:04:56.165366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport torch.nn as nn\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\nfrom train import Training\n\nmini_batch_size = 128 #intentar 32,64 \nhalf_minibatch = mini_batch_size/2\nN_mini_batch = 130\ntotal_traject = mini_batch_size * N_mini_batch\ntrain_traject = int(total_traject * 0.7)\ntrain_traject2 = train_traject/2 \ntest_traject = int(total_traject * 0.3)\nN_despl = 20\nN_coord = 2\nlimit_arena = 1\nlimit_arena2 = -1\ngrad_90 = np.pi\n\nimport time\nstart_time = time.time()\nt_traject = train_traject2\n## disable cudnn fixed the mem leak problem\ntorch.backends.cudnn.enabled = False\n        \ntrain = Training()\nall_losses, all_activation1, all_activation2, all_activation_soft1, all_activation_soft2, batch_y1, y2, my_network  = train.train_model()\n# fig, ax = plt.subplots (2, 2)\n# ax[0, 0].plot (all_losses[1:], c='b', label='total')\n# ax[0, 0].set_title ('total loss')\n# ax[0, 1].plot (all_losses1[1:], c='g', label='MSEloss')\n# ax[0, 1].set_title ('MSEloss')\n# ax[1, 0].plot (all_losses2[1:], c='r', label='firing')\n# ax[1, 0].set_title ('firing')\n# plt.show ()\n\ntorch.save(my_network,'/kaggle/working/MyNetworkKaggle5.tar')\n\nlinear_size = t_traject * N_despl\n\ndata_x = batch_y1[:,:, 0].cpu().detach().numpy().T\ndata_y = batch_y1[:,:, 1].cpu().detach().numpy().T\n# data[20,224] or [32,20] tiempo,N_traject\n\n\ndata_x = np.reshape(data_x, -1, order='F')\ndata_y = np.reshape(data_y, -1, order='F')\nprint('done')\ntimedelta = time.time() - start_time\n\nprint(f\"The program took {timedelta} seconds to run\")\t","metadata":{"execution":{"iopub.status.busy":"2023-09-11T18:04:56.166884Z","iopub.status.idle":"2023-09-11T18:04:56.167322Z","shell.execute_reply.started":"2023-09-11T18:04:56.167091Z","shell.execute_reply":"2023-09-11T18:04:56.167114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"linear_size = t_traject * N_despl\n\ndata_x = batch_y1[:,:, 0].cpu().detach().numpy().T\ndata_y = batch_y1[:,:, 1].cpu().detach().numpy().T\n# data[20,224] or [32,20] tiempo,N_traject\n\n\ndata_x = np.reshape(data_x, -1, order='F')\ndata_y = np.reshape(data_y, -1, order='F')\n\nfig, ax = plt.subplots(1,1)\nax.hist2d(data_x, data_y, bins=20, range=[[-np.max(data_x), np.max(data_x)], [-np.max(data_y), np.max(data_y)]])\nplt.title('Todas las trayectorias')\nplt.savefig('all_trajects.png')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-11T18:04:56.168868Z","iopub.status.idle":"2023-09-11T18:04:56.169306Z","shell.execute_reply.started":"2023-09-11T18:04:56.169079Z","shell.execute_reply":"2023-09-11T18:04:56.169101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hist, *edges = np.histogram2d(data_x, data_y, bins=20, range=[[-np.max(data_x), np.max(data_x)], [-np.max(data_y), np.max(data_y)]])\ninx = np.digitize(data_x, edges[0], right=True)\niny = np.digitize(data_y, edges[1], right=True)","metadata":{"execution":{"iopub.status.busy":"2023-09-11T18:04:56.171089Z","iopub.status.idle":"2023-09-11T18:04:56.171485Z","shell.execute_reply.started":"2023-09-11T18:04:56.171323Z","shell.execute_reply":"2023-09-11T18:04:56.171341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"axisbins = list(zip(inx, iny))\naxisbins = np.array(axisbins)\n\nfig, ax = plt.subplots(1,1)\nax.plot(y2[-1,:,0].cpu().detach().numpy(), y2[-1,:,1].detach().cpu().numpy(), c = 'b', label = 'modelo', marker = '.')\nax.plot(batch_y1[-1,:,0].cpu().detach().numpy(), batch_y1[-1,:,1].cpu().detach().numpy(), c = 'r', label = 'real', marker = '.')\nplt.legend()\nplt.savefig('traject_example.png')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-11T18:04:56.172807Z","iopub.status.idle":"2023-09-11T18:04:56.173118Z","shell.execute_reply.started":"2023-09-11T18:04:56.172973Z","shell.execute_reply":"2023-09-11T18:04:56.172988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"last_loss = all_losses[-1]\nprint(last_loss)","metadata":{"execution":{"iopub.status.busy":"2023-09-11T18:04:56.174140Z","iopub.status.idle":"2023-09-11T18:04:56.174450Z","shell.execute_reply.started":"2023-09-11T18:04:56.174303Z","shell.execute_reply":"2023-09-11T18:04:56.174318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"diferencia_array = np.zeros((2, y2[:,0,0].cpu().detach().numpy().size))\n\nfor i in range(y2[:,0,0].cpu().detach().numpy().size):\n    x_dis, y_dis = distancia_modelo_real(y2[i, :, :].cpu().detach().numpy(), batch_y1[i, :, :].cpu().detach().numpy())\n    diferencia_array[:,i] = [x_dis, y_dis]\n\nplt.hist(diferencia_array[0,:], bins=100, range=[0, 1])\nplt.gca().set(title='Histograma de distancias', ylabel='Frequencia')\nplt.savefig('distance.png')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-11T18:04:56.175690Z","iopub.status.idle":"2023-09-11T18:04:56.175978Z","shell.execute_reply.started":"2023-09-11T18:04:56.175839Z","shell.execute_reply":"2023-09-11T18:04:56.175853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"act_array = np.array(all_activation1.cpu().detach().numpy())\nact_array1 = np.reshape(act_array, [linear_size, N_hidden_neurons], order = 'C')\n\nfinal_array = np.zeros([20,20,N_hidden_neurons])\ncounter = np.zeros([20,20])\ncounter = np.expand_dims(counter, 2)\n\nfor n in range(linear_size):\n    xbin = axisbins[n,0]\n    ybin = axisbins[n,1]\n    final_array[xbin-1,ybin-1,:] += act_array1[n,:]\n    counter[xbin-1,ybin-1] += 1\n\nplt.plot(all_losses[1:].cpu().detach().numpy(), c = 'b', label = 'total')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.savefig('loss1.png')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-11T18:04:56.176950Z","iopub.status.idle":"2023-09-11T18:04:56.177226Z","shell.execute_reply.started":"2023-09-11T18:04:56.177090Z","shell.execute_reply":"2023-09-11T18:04:56.177103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, [[ax1, ax2], [ax3, ax4]] = plt.subplots(nrows=2, ncols=2)\n\nax1.plot(y2[0,:,0].cpu().detach().numpy(), y2[0,:,1].detach().cpu().numpy(), c = 'b', label = 'modelo', marker = '.')\nax1.plot(batch_y1[0,:,0].cpu().detach().numpy(), batch_y1[0,:,1].cpu().detach().numpy(), c = 'r', label = 'real', marker = '.')\nplt.legend()\n\nax2.plot(y2[1,:,0].cpu().detach().numpy(), y2[1,:,1].detach().cpu().numpy(), c = 'b', label = 'modelo', marker = '.')\nax2.plot(batch_y1[1,:,0].cpu().detach().numpy(), batch_y1[1,:,1].cpu().detach().numpy(), c = 'r', label = 'real', marker = '.')\nplt.legend()\n\nax3.plot(y2[2,:,0].cpu().detach().numpy(), y2[2,:,1].detach().cpu().numpy(), c = 'b', label = 'modelo', marker = '.')\nax3.plot(batch_y1[2,:,0].cpu().detach().numpy(), batch_y1[2,:,1].cpu().detach().numpy(), c = 'r', label = 'real', marker = '.')\nplt.legend()\n\nax4.plot(y2[3,:,0].cpu().detach().numpy(), y2[3,:,1].detach().cpu().numpy(), c = 'b', label = 'modelo', marker = '.')\nax4.plot(batch_y1[3,:,0].cpu().detach().numpy(), batch_y1[3,:,1].cpu().detach().numpy(), c = 'r', label = 'real', marker = '.')\nplt.legend()\nplt.savefig('traject_examples.png')","metadata":{"execution":{"iopub.status.busy":"2023-09-11T18:04:56.181589Z","iopub.status.idle":"2023-09-11T18:04:56.182180Z","shell.execute_reply.started":"2023-09-11T18:04:56.181913Z","shell.execute_reply":"2023-09-11T18:04:56.181941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(all_losses[125:].cpu().detach().numpy(), c = 'b', label = 'total')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.savefig('loss.png')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-11T18:04:56.183137Z","iopub.status.idle":"2023-09-11T18:04:56.183606Z","shell.execute_reply.started":"2023-09-11T18:04:56.183379Z","shell.execute_reply":"2023-09-11T18:04:56.183401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"N_softmax_neurons = 200\nact_array_soft = np.array(all_activation_soft1.cpu().detach().numpy())\nact_array2 = np.reshape(act_array_soft, [linear_size, N_softmax_neurons], order = 'C')\n\nfinal_array_soft = np.zeros([20,20,N_softmax_neurons])\ncounter_soft = np.zeros([20,20])\ncounter_soft = np.expand_dims(counter_soft, 2)\n\nfor n in range(linear_size):\n    xbin = axisbins[n,0]\n    ybin = axisbins[n,1]\n    final_array_soft[xbin-1,ybin-1,:] += act_array2[n,:]\n    counter_soft[xbin-1,ybin-1] += 1\nprint('done')","metadata":{"execution":{"iopub.status.busy":"2023-09-11T18:04:56.185152Z","iopub.status.idle":"2023-09-11T18:04:56.185612Z","shell.execute_reply.started":"2023-09-11T18:04:56.185384Z","shell.execute_reply":"2023-09-11T18:04:56.185408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_array5 = final_array / counter\nfinal_array6 = final_array5[:,:,2]\nplt.imshow(final_array6)\nplt.colorbar()","metadata":{"execution":{"iopub.status.busy":"2023-09-11T18:04:56.187074Z","iopub.status.idle":"2023-09-11T18:04:56.187548Z","shell.execute_reply.started":"2023-09-11T18:04:56.187313Z","shell.execute_reply":"2023-09-11T18:04:56.187336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"N_hidden_neurons = 100\nplt.figure(figsize=(20, 17))\nfor n in range(N_hidden_neurons):\n    ax = plt.subplot(10, 10, n + 1)\n    ax.imshow(final_array[:,:,n])\nprint('Hidden layer')\nplt.savefig('hidden_neurons.png')\n\n#Var(Time, batch)   activaciones recurrente\n#Ruido\n\n#Mayor beta, ruido en la softmax tambin, =/= n neuronas","metadata":{"execution":{"iopub.status.busy":"2023-09-11T18:04:56.188985Z","iopub.status.idle":"2023-09-11T18:04:56.189460Z","shell.execute_reply.started":"2023-09-11T18:04:56.189224Z","shell.execute_reply":"2023-09-11T18:04:56.189247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_arrayplot = final_array / counter\nplt.figure(figsize=(20, 17))\nfor n in range(N_hidden_neurons):\n    ax = plt.subplot(10, 10, n + 1)\n    ax.imshow(final_arrayplot[:,:,n], vmin=0, vmax=1)\nprint('Hidden layer')\nplt.savefig('hidden_neurons1.png')\n\n\n#discriminar las activaciones de un contexto y otro\n#cuantificar si error en contexto 1 de estimacion con respecto a \"realidad\" del otro contexto es mayor al error de estimacion del mismo contexto\n#verificar que esta haciendo la softmax","metadata":{"execution":{"iopub.status.busy":"2023-09-11T18:04:56.190811Z","iopub.status.idle":"2023-09-11T18:04:56.191280Z","shell.execute_reply.started":"2023-09-11T18:04:56.191033Z","shell.execute_reply":"2023-09-11T18:04:56.191055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"N_softmax_neurons = 200\nfinal_array_soft = final_array_soft / counter_soft\nfinal_array2 = final_array_soft[:,:,0]\nplt.figure(figsize=(40, 35))\nfor n in range(N_softmax_neurons):\n    ax = plt.subplot(20, 10, n + 1)\n    ax.imshow(final_array_soft[:,:,n])\nprint('Softmax layer')\nplt.savefig('softmax_neurons.png')","metadata":{"execution":{"iopub.status.busy":"2023-09-11T18:04:56.192800Z","iopub.status.idle":"2023-09-11T18:04:56.193270Z","shell.execute_reply.started":"2023-09-11T18:04:56.193024Z","shell.execute_reply":"2023-09-11T18:04:56.193046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sum(final_array_soft[0,0,:])","metadata":{"execution":{"iopub.status.busy":"2023-09-11T18:04:56.194559Z","iopub.status.idle":"2023-09-11T18:04:56.197179Z","shell.execute_reply.started":"2023-09-11T18:04:56.196957Z","shell.execute_reply":"2023-09-11T18:04:56.196983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(final_array_soft[:,:,0], vmin=0, vmax=1)\nplt.colorbar()\n","metadata":{"execution":{"iopub.status.busy":"2023-09-11T18:04:56.198320Z","iopub.status.idle":"2023-09-11T18:04:56.198624Z","shell.execute_reply.started":"2023-09-11T18:04:56.198477Z","shell.execute_reply":"2023-09-11T18:04:56.198493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sum(final_array_soft[0,0,:])","metadata":{"execution":{"iopub.status.busy":"2023-09-11T18:04:56.199859Z","iopub.status.idle":"2023-09-11T18:04:56.200396Z","shell.execute_reply.started":"2023-09-11T18:04:56.200224Z","shell.execute_reply":"2023-09-11T18:04:56.200250Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sum(counter_soft[0,0,:])","metadata":{"execution":{"iopub.status.busy":"2023-09-11T18:04:56.201409Z","iopub.status.idle":"2023-09-11T18:04:56.201961Z","shell.execute_reply.started":"2023-09-11T18:04:56.201801Z","shell.execute_reply":"2023-09-11T18:04:56.201818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_array_soft = np.zeros([20,20,N_softmax_neurons])\ncounter_soft = np.zeros([20,20])\ncounter_soft = np.expand_dims(counter_soft, 2)\n\nfor n in range(linear_size):\n    xbin = axisbins[n,0]\n    ybin = axisbins[n,1]\n    final_array_soft[xbin-1,ybin-1,:] += act_array2[n,:]\n    counter_soft[xbin-1,ybin-1] += 1\nprint('done')","metadata":{"execution":{"iopub.status.busy":"2023-09-11T18:04:56.202958Z","iopub.status.idle":"2023-09-11T18:04:56.203480Z","shell.execute_reply.started":"2023-09-11T18:04:56.203322Z","shell.execute_reply":"2023-09-11T18:04:56.203340Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_array_softmax = final_array_soft / counter_soft\nplt.figure(figsize=(40, 35))\nfor n in range(N_softmax_neurons):\n    ax = plt.subplot(20, 10, n + 1)\n    ax.imshow(final_array_softmax[:,:,n], vmin=0, vmax=0.5)\nprint('Softmax layer')\nplt.savefig('softmax_neurons1.png')\n","metadata":{"execution":{"iopub.status.busy":"2023-09-11T18:04:56.204443Z","iopub.status.idle":"2023-09-11T18:04:56.205021Z","shell.execute_reply.started":"2023-09-11T18:04:56.204856Z","shell.execute_reply":"2023-09-11T18:04:56.204874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"peso = my_network.linear3.weight\npeso2 = peso.cpu().detach().numpy().flatten()\nplt.hist(peso2)","metadata":{"execution":{"iopub.status.busy":"2023-09-11T18:04:56.205884Z","iopub.status.idle":"2023-09-11T18:04:56.206187Z","shell.execute_reply.started":"2023-09-11T18:04:56.206034Z","shell.execute_reply":"2023-09-11T18:04:56.206050Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"peso = my_network.linear5.weight\npeso1 = peso.cpu().detach().numpy()\nplt.plot(peso1.T[:,0], peso1.T[:,1], \".\")","metadata":{"execution":{"iopub.status.busy":"2023-09-11T18:04:56.207418Z","iopub.status.idle":"2023-09-11T18:04:56.207757Z","shell.execute_reply.started":"2023-09-11T18:04:56.207565Z","shell.execute_reply":"2023-09-11T18:04:56.207581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"peso = my_network.linear5.weight\npeso1 = peso.cpu().detach().numpy().flatten()\nplt.hist(peso1)","metadata":{"execution":{"iopub.status.busy":"2023-09-11T18:04:56.209226Z","iopub.status.idle":"2023-09-11T18:04:56.209536Z","shell.execute_reply.started":"2023-09-11T18:04:56.209381Z","shell.execute_reply":"2023-09-11T18:04:56.209397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"peso = my_network.linear4.weight\npeso1 = peso.cpu().detach().numpy()\nplt.plot(peso1.T[:,0], peso1.T[:,1], \"x\")","metadata":{"execution":{"iopub.status.busy":"2023-09-11T18:04:56.210701Z","iopub.status.idle":"2023-09-11T18:04:56.211020Z","shell.execute_reply.started":"2023-09-11T18:04:56.210863Z","shell.execute_reply":"2023-09-11T18:04:56.210878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, [[ax1, ax2], [ax3, ax4]] = plt.subplots(nrows=2, ncols=2)\n\nax1.plot(y2[50,:,0].cpu().detach().numpy(), y2[50,:,1].detach().cpu().numpy(), c = 'b', label = 'modelo', marker = '.')\nax1.plot(batch_y1[50,:,0].cpu().detach().numpy(), batch_y1[50,:,1].cpu().detach().numpy(), c = 'r', label = 'real', marker = '.')\nplt.legend()\n\nax2.plot(y2[51,:,0].cpu().detach().numpy(), y2[51,:,1].detach().cpu().numpy(), c = 'b', label = 'modelo', marker = '.')\nax2.plot(batch_y1[51,:,0].cpu().detach().numpy(), batch_y1[51,:,1].cpu().detach().numpy(), c = 'r', label = 'real', marker = '.')\nplt.legend()\n\nax3.plot(y2[52,:,0].cpu().detach().numpy(), y2[52,:,1].detach().cpu().numpy(), c = 'b', label = 'modelo', marker = '.')\nax3.plot(batch_y1[52,:,0].cpu().detach().numpy(), batch_y1[52,:,1].cpu().detach().numpy(), c = 'r', label = 'real', marker = '.')\nplt.legend()\n\nax4.plot(y2[53,:,0].cpu().detach().numpy(), y2[53,:,1].detach().cpu().numpy(), c = 'b', label = 'modelo', marker = '.')\nax4.plot(batch_y1[53,:,0].cpu().detach().numpy(), batch_y1[53,:,1].cpu().detach().numpy(), c = 'r', label = 'real', marker = '.')\nplt.legend()","metadata":{"execution":{"iopub.status.busy":"2023-09-11T18:04:56.211718Z","iopub.status.idle":"2023-09-11T18:04:56.212019Z","shell.execute_reply.started":"2023-09-11T18:04:56.211870Z","shell.execute_reply":"2023-09-11T18:04:56.211887Z"},"trusted":true},"execution_count":null,"outputs":[]}]}